{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1fNvP68r8H1on5RaJ0Aq6WcS9vvEkxpvL","timestamp":1662444999587},{"file_id":"1Xrh4MFYbJUFQnZ0JWyvKFcj4Wh60mPK4","timestamp":1662387501770}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","source":["!pip install wandb"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gV7Eul5O5K-P","outputId":"4ef18b7d-292b-4246-b2fa-46f79f7edf9d","executionInfo":{"status":"ok","timestamp":1662480756321,"user_tz":-540,"elapsed":13143,"user":{"displayName":"김성찬","userId":"03517507033102912155"}}},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting wandb\n","  Downloading wandb-0.13.2-py2.py3-none-any.whl (1.8 MB)\n","\u001b[K     |████████████████████████████████| 1.8 MB 8.9 MB/s \n","\u001b[?25hCollecting sentry-sdk>=1.0.0\n","  Downloading sentry_sdk-1.9.8-py2.py3-none-any.whl (158 kB)\n","\u001b[K     |████████████████████████████████| 158 kB 62.0 MB/s \n","\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from wandb) (57.4.0)\n","Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n","Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n","Collecting docker-pycreds>=0.4.0\n","  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n","Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n","Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n","Collecting shortuuid>=0.5.0\n","  Downloading shortuuid-1.0.9-py3-none-any.whl (9.4 kB)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (6.0)\n","Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n","Collecting setproctitle\n","  Downloading setproctitle-1.3.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n","Requirement already satisfied: protobuf<4.0dev,>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n","Collecting pathtools\n","  Downloading pathtools-0.1.2.tar.gz (11 kB)\n","Collecting GitPython>=1.0.0\n","  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n","\u001b[K     |████████████████████████████████| 181 kB 53.2 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.1.1)\n","Collecting gitdb<5,>=4.0.1\n","  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n","\u001b[K     |████████████████████████████████| 63 kB 1.4 MB/s \n","\u001b[?25hCollecting smmap<6,>=3.0.1\n","  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2022.6.15)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n","Collecting sentry-sdk>=1.0.0\n","  Downloading sentry_sdk-1.9.7-py2.py3-none-any.whl (157 kB)\n","\u001b[K     |████████████████████████████████| 157 kB 74.9 MB/s \n","\u001b[?25h  Downloading sentry_sdk-1.9.6-py2.py3-none-any.whl (157 kB)\n","\u001b[K     |████████████████████████████████| 157 kB 79.7 MB/s \n","\u001b[?25h  Downloading sentry_sdk-1.9.5-py2.py3-none-any.whl (157 kB)\n","\u001b[K     |████████████████████████████████| 157 kB 68.7 MB/s \n","\u001b[?25h  Downloading sentry_sdk-1.9.4-py2.py3-none-any.whl (157 kB)\n","\u001b[K     |████████████████████████████████| 157 kB 72.0 MB/s \n","\u001b[?25h  Downloading sentry_sdk-1.9.3-py2.py3-none-any.whl (157 kB)\n","\u001b[K     |████████████████████████████████| 157 kB 67.5 MB/s \n","\u001b[?25h  Downloading sentry_sdk-1.9.2-py2.py3-none-any.whl (157 kB)\n","\u001b[K     |████████████████████████████████| 157 kB 68.3 MB/s \n","\u001b[?25h  Downloading sentry_sdk-1.9.1-py2.py3-none-any.whl (157 kB)\n","\u001b[K     |████████████████████████████████| 157 kB 62.4 MB/s \n","\u001b[?25h  Downloading sentry_sdk-1.9.0-py2.py3-none-any.whl (156 kB)\n","\u001b[K     |████████████████████████████████| 156 kB 70.8 MB/s \n","\u001b[?25hBuilding wheels for collected packages: pathtools\n","  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=a529c73903393553f3f9ea81b7c44766808f6d63a63fafd89accd56ad8a6d6da\n","  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n","Successfully built pathtools\n","Installing collected packages: smmap, gitdb, shortuuid, setproctitle, sentry-sdk, pathtools, GitPython, docker-pycreds, wandb\n","Successfully installed GitPython-3.1.27 docker-pycreds-0.4.0 gitdb-4.0.9 pathtools-0.1.2 sentry-sdk-1.9.0 setproctitle-1.3.2 shortuuid-1.0.9 smmap-5.0.0 wandb-0.13.2\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h4k7RepxdLZb","executionInfo":{"status":"ok","timestamp":1662480775244,"user_tz":-540,"elapsed":18928,"user":{"displayName":"김성찬","userId":"03517507033102912155"}},"outputId":"0f261697-022d-428f-e7b9-fc75bf05d1c1"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["%ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yiHYgCkfdX5F","executionInfo":{"status":"ok","timestamp":1662480775245,"user_tz":-540,"elapsed":8,"user":{"displayName":"김성찬","userId":"03517507033102912155"}},"outputId":"82dbe7fc-2e25-4367-f8a5-2af7757ff165"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[0m\u001b[01;34mdrive\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"]}]},{"cell_type":"code","source":["%cd drive/MyDrive/data/aT/scaled_data/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tLejx7u_uvjP","executionInfo":{"status":"ok","timestamp":1662480781362,"user_tz":-540,"elapsed":757,"user":{"displayName":"김성찬","userId":"03517507033102912155"}},"outputId":"ea255c3e-cc3e-4d26-86db-797752248b88"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/data/aT/scaled_data\n"]}]},{"cell_type":"code","source":["%ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3poS_XLa23Rt","executionInfo":{"status":"ok","timestamp":1662480783137,"user_tz":-540,"elapsed":803,"user":{"displayName":"김성찬","userId":"03517507033102912155"}},"outputId":"809b9a12-f3b1-4a75-d35b-1f99a1302b4e"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[0m\u001b[01;34maT_test_raw\u001b[0m/  \u001b[01;34mresult_001\u001b[0m/  sutmit_005.csv  \u001b[01;34mtest\u001b[0m/  \u001b[01;34mtrain\u001b[0m/  \u001b[01;34mwandb\u001b[0m/\n"]}]},{"cell_type":"code","source":["!wandb login"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yFwtYiBE5cgU","executionInfo":{"status":"ok","timestamp":1662480804472,"user_tz":-540,"elapsed":15356,"user":{"displayName":"김성찬","userId":"03517507033102912155"}},"outputId":"438121b7-605b-4260-f066-67782335486e"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.optim.lr_scheduler as lr_scheduler\n","from torch.utils.data import Dataset, DataLoader\n","\n","import time\n","import wandb\n","import pandas as pd\n","import numpy as np\n","from tqdm import tqdm\n","import warnings\n","from glob import glob\n","from sklearn.model_selection import train_test_split\n","import random\n","import os\n","\n","# 경고 끄기\n","warnings.filterwarnings(action='ignore')\n","\n","# 시드고정\n","torch.random.manual_seed(42)\n","random.seed(42)\n","np.random.seed(42)"],"metadata":{"id":"DxBU2KYFrjgv","executionInfo":{"status":"ok","timestamp":1662480815143,"user_tz":-540,"elapsed":3,"user":{"displayName":"김성찬","userId":"03517507033102912155"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["data_list = glob('.//train/*.csv')\n","epoch = 100\n","batch = 15\n","tr_del_list = ['단가(원)', '거래량', '거래대금(원)', '경매건수', '도매시장코드', '도매법인코드', '산지코드 '] # train 에서 사용하지 않는 열\n","ts_del_list = ['단가(원)', '거래량', '거래대금(원)', '경매건수', '도매시장코드', '도매법인코드', '산지코드 ', '해당일자_전체평균가격(원)'] # test 에서 사용하지 않는 열\n","check_col = ['일자구분_중순', '일자구분_초순', '일자구분_하순','월구분_10월', '월구분_11월', '월구분_12월', '월구분_1월', '월구분_2월', '월구분_3월', \n","             '월구분_4월','월구분_5월', '월구분_6월', '월구분_7월', '월구분_8월', '월구분_9월'] # 열 개수 맞추기"],"metadata":{"id":"O1og1-aHuxHb","executionInfo":{"status":"ok","timestamp":1662480820331,"user_tz":-540,"elapsed":294,"user":{"displayName":"김성찬","userId":"03517507033102912155"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["def time_window(df, t, t_sep):\n","    seq_len = t\n","    seqence_length = seq_len + t_sep\n","\n","    result = []\n","    for index in range(len(df) - seqence_length):\n","        tmp = df[index: index + seqence_length].values\n","        tmp = np.vstack(tmp).astype(np.float)\n","        tmp = torch.from_numpy(tmp)\n","        result.append(tmp)\n","\n","    return np.array(result)\n","\n","\n","def make_dataset(i):\n","    df_number = i.split(\"_\")[-1].split(\".\")[0]\n","    df = pd.read_csv(i)\n","\n","    for j in df.columns:\n","        df[j] = df[j].replace({' ': np.nan})\n","\n","    # 사용할 열 선택 및 index 설정\n","    df.drop(tr_del_list, axis=1, inplace=True)\n","    df.set_index('datadate', drop=True, inplace=True)\n","\n","    # nan 처리\n","    df = df.fillna(0)\n","\n","    # 변수와 타겟 분리\n","    x, y = df[[i for i in df.columns if i != '해당일자_전체평균가격(원)']], df['해당일자_전체평균가격(원)']\n","\n","    # 2주 입력을 통한 이후 4주 예측을 위해 y의 첫 14일을 제외\n","    y = y[14:]\n","\n","    # time series window 생성\n","    data_x = time_window(x, 13, 1)\n","    data_y = time_window(y, 27, 1)\n","\n","    # y의 길이와 같은 길이로 설정\n","    xdata = data_x[:len(data_y)]\n","    ydata = data_y\n","\n","    return xdata, ydata\n","\n","\n","class windowDataset(Dataset):\n","    def __init__(self, data):\n","        self.xdata, self.ydata = make_dataset(data)\n","\n","    def __len__(self):\n","        return len(self.xdata)\n","\n","    def __getitem__(self, idx):\n","        return self.xdata[idx], self.ydata[idx].reshape(-1)"],"metadata":{"id":"Lrej9_OyvVms","executionInfo":{"status":"ok","timestamp":1662480820616,"user_tz":-540,"elapsed":4,"user":{"displayName":"김성찬","userId":"03517507033102912155"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["class Flag:\n","    def __init__(self, flags):\n","        for key, value in flags.items():\n","            if isinstance(value, dict):\n","                self.__dict__[key] = Flag(value)\n","            else:\n","                self.__dict__[key] = value"],"metadata":{"id":"VhKJjPQgxcnb","executionInfo":{"status":"ok","timestamp":1662480822357,"user_tz":-540,"elapsed":278,"user":{"displayName":"김성찬","userId":"03517507033102912155"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["def make_Tensor(array):\n","    return torch.from_numpy(array)\n","\n","\n","def astype_data(data):\n","    df = data.astype(np.float32)\n","    return make_Tensor(df)\n","\n","\n","class testDataset(Dataset):\n","    def __init__(self, data):\n","        zero_csv = [0 for i in range(14)]\n","        df = pd.read_csv(data)\n","\n","        if len(df) == 0:\n","            print('no data in Dataset!!')\n","            print(df)\n","            df['zero_non'] = zero_csv\n","            print(df)\n","            df = df.fillna(0)\n","            print(df)\n","            df.drop('zero_non', axis=1, inplace=True)\n","            df.drop('Unnamed: 0', axis=1, inplace=True)\n","            print(df)\n","\n","        file_number = data.split('test_')[1].split('.')[0]\n","\n","        # 사용할 열 선택, index 설정\n","        df.drop(ts_del_list, axis=1, inplace=True)\n","        df.set_index('datadate', drop=True, inplace=True)\n","\n","        # train input 과 형상 맞추기\n","        add_col = [i for i in check_col if i not in df.columns]\n","\n","        for a in add_col:\n","            df[a] = 0\n","\n","        # ' ' -> nan 으로 변경\n","        for a in df.columns:\n","            df[a] = df[a].replace({' ': np.nan})\n","\n","        # nan 처리\n","        df = df.fillna(0)\n","\n","        # x_test  생성\n","        self.df_test = astype_data(df.values.reshape(1, df.values.shape[0], df.values.shape[1]))\n","\n","    def __len__(self):\n","        return len(self.df_test)\n","\n","    def __getitem__(self, idx):\n","        return self.df_test[idx]\n","\n","# sampletestset = testDataset('/content/drive/MyDrive/Colab Notebooks/농산물/preprocess/test/set_0/test_0.csv')\n","# sampletestloader = DataLoader(sampletestset, batch_size = 1, shuffle=False)\n","\n","# for (i, o) in sampletestloader:\n","#     print(i.shape, o.shape)\n","#     print(i)\n","#     print(o)\n","#     break"],"metadata":{"id":"jxDFo0mRVnqN","executionInfo":{"status":"ok","timestamp":1662480826523,"user_tz":-540,"elapsed":394,"user":{"displayName":"김성찬","userId":"03517507033102912155"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["def train(flags, idx):\n","    name = f'Exp_X_U_batch_128_{flags[\"data_num\"]:03d}_{idx}'\n","    \n","    flags['data_path'] = f'./train/train_{flags[\"data_num\"]}.csv'\n","\n","    wandb.init(\n","        project=\"Nong_Linear\", \n","        entity=\"deep-overflow\", \n","        config=flags,\n","        name=name\n","    )\n","\n","    # Flag # ====================\n","    # 다양한 Regularization 시도해보기\n","    # ===========================\n","    flags = Flag(flags)\n","\n","    # Dataset # ====================\n","    train_dataset = windowDataset(flags.data_path)\n","    test_dataset = testDataset()\n","\n","    # DataLoader # ====================\n","    train_dataloader = DataLoader(\n","        dataset=train_dataset,\n","        batch_size=flags.batch_size,\n","        shuffle=True\n","    )\n","\n","    # Net # ====================\n","    # Dropout\n","    # BatchNorm\n","    # ==========================\n","    net = nn.Sequential(\n","        nn.Flatten(),\n","        nn.Linear(14 * 49, flags.model.hid_dim),\n","        # nn.BatchNorm1d(flags.model.hid_dim),\n","        nn.LeakyReLU(),\n","        nn.Dropout(flags.dropout),\n","    )\n","\n","    for _ in range(flags.model.nlayers - 2):\n","        net.append(nn.Linear(flags.model.hid_dim, flags.model.hid_dim))\n","        # net.append(nn.BatchNorm1d(flags.model.hid_dim))\n","        net.append(nn.LeakyReLU())\n","        net.append(nn.Dropout(flags.dropout))\n","    \n","    net.append(nn.Linear(flags.model.hid_dim, 28))\n","    net.append(nn.ReLU())\n","\n","    # Weight Initialization # ====================\n","    # Xavier\n","    # Kaiming\n","    # ============================================\n","    for name, param in net.named_parameters():\n","        if name.split('.')[-1] == 'bias':\n","            continue\n","        print(f'Init {name}')\n","        nn.init.xavier_uniform_(param)\n","        # nn.init.kaiming_uniform_(param, nonlinearity='leaky_relu')\n","\n","    # Criterion # ====================\n","    # 다양한 로스 시도해보기\n","    # ================================\n","    criterion = nn.L1Loss()\n","\n","    # Optimizer # ====================\n","    # 다양한 옵티마이저 시도해보기\n","    # ================================\n","    optimizer = optim.Adam(\n","        params=net.parameters(), \n","        lr=flags.lr,\n","        betas=(0.9, 0.999),\n","    )\n","\n","    # scheduler = lr_scheduler.ExponentialLR(\n","    #     optimizer=optimizer,\n","    #     gamma=0.9\n","    # )\n","\n","    # scheduler = lr_scheduler.StepLR(\n","    #     optimizer=optimizer,\n","    #     step_size=flags.lr_scheduler.step_size,\n","    #     gamma=flags.lr_scheduler.gamma\n","    # )\n","\n","    def lr_schedule_fn(epoch):\n","        if epoch < 15:\n","            return 1.5 # 1e-4 * 1.5\n","        elif epoch >= 15 and epoch < 250:\n","            return 1.0\n","        elif epoch >= 250 and epoch < 350:\n","            return 1.5\n","        else:\n","            return 1.0\n","\n","\n","    # =====\n","    # a: 0 - 14: 1.5 // 15 - 249: 1 // 250 - 399: 1.5 // 400 - 499: 1 (O)\n","    # b: 0 - 14: 1.5 // 15 - 249: 1 // 250 - 349: 2 // 350 - 499: 1\n","    # c: 0 - 14: 1.5 // 15 - 249: 1 // 250 - 349: 10 // 350 - 499: 1 (X)\n","    # d: 0 - 14: 1.5 // 15 - 249: 1 // 250 - 349: 3 // 350 - 499: 1\n","    \n","    scheduler = lr_scheduler.LambdaLR(\n","        optimizer=optimizer,\n","        lr_lambda=lr_schedule_fn\n","    )\n","\n","    # scheduler = lr_scheduler.CosineAnnealingLR(\n","    #     optimizer=optimizer,\n","    #     T_max=100\n","    # )\n","\n","    # Device\n","    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","    net = net.to(device)\n","\n","    # Train\n","    for epoch in range(flags.epochs):\n","            \n","        epoch_train_loss = 0.0\n","\n","        for inputs, labels in train_dataloader:\n","            inputs, labels = inputs.float().to(device), labels.to(device)\n","\n","            with torch.set_grad_enabled(True):\n","                outputs = net(inputs)\n","                loss = criterion(outputs, labels)\n","\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","        \n","            epoch_train_loss += loss.item()\n","        \n","        scheduler.step()\n","        \n","        wandb.log({\n","            'loss': epoch_train_loss / len(train_dataloader),\n","            'lr': optimizer.param_groups[0]['lr']\n","        })\n","        if (epoch + 1) % 10 == 0:\n","            print(f'[epoch : {epoch + 1} / {flags.epochs}] Train Loss : {epoch_train_loss / len(train_dataloader)}')\n","\n","for i in range(1):\n","    flags = {\n","        'epochs': 500,\n","        'lr': 1e-4,\n","        'batch_size': 64,\n","        'data_num': 0,\n","        'model': {\n","            'nlayers': 10,\n","            'hid_dim': 1024,\n","            'weight_init': 'xavier_uniform'\n","        },\n","        'optim': 'Adam',\n","        'criterion': 'L1Loss',\n","        'dropout': 0.5,\n","        'lr_scheduler': {\n","            'method': 'Lambda'\n","        }\n","    }\n","\n","    train(flags, i)"],"metadata":{"id":"GjVzyrlPziGt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train_and_test(flags):\n","    name = 'Test_001'\n","\n","    wandb.init(\n","        project=\"Nong_Linear\",\n","        entity='deep-overflow',\n","        config=flags,\n","        name=name\n","    )\n","\n","    flags = Flag(flags)\n","\n","    for item_idx in range(37): # 37로 수정하기\n","        \n","        data_path = f'./train/train_{item_idx}.csv'\n","        train_dataset = windowDataset(data_path)\n","\n","        train_dataloader = DataLoader(\n","            dataset=train_dataset,\n","            batch_size=flags.batch_size,\n","            shuffle=True,\n","        )\n","\n","        net = nn.Sequential(\n","            nn.Flatten(),\n","            nn.Linear(14 * 49, flags.model.hid_dim),\n","            # nn.BatchNorm1d(flags.model.hid_dim),\n","            nn.LeakyReLU(),\n","            nn.Dropout(flags.dropout),\n","        )\n","\n","        for _ in range(flags.model.nlayers - 2):\n","            net.append(nn.Linear(flags.model.hid_dim, flags.model.hid_dim))\n","            # net.append(nn.BatchNorm1d(flags.model.hid_dim))\n","            net.append(nn.LeakyReLU())\n","            net.append(nn.Dropout(flags.dropout))\n","        \n","        net.append(nn.Linear(flags.model.hid_dim, 28))\n","        net.append(nn.ReLU())\n","\n","        for name, param in net.named_parameters():\n","            if name.split('.')[-1] == 'bias':\n","                continue\n","            nn.init.xavier_uniform_(param)\n","            # nn.init.kaiming_uniform_(param, nonlinearity='leaky_relu')\n","\n","        criterion = nn.L1Loss()\n","\n","        optimizer = optim.Adam(\n","            params=net.parameters(), \n","            lr=flags.lr,\n","            betas=(0.9, 0.999),\n","        )\n","\n","        def lr_schedule_fn(epoch):\n","            if epoch < 15:\n","                return 1.5 # 1e-4 * 1.5\n","            elif epoch >= 15 and epoch < 250:\n","                return 1.0\n","            elif epoch >= 250 and epoch < 350:\n","                return 1.5\n","            else:\n","                return 1.0\n","\n","        scheduler = lr_scheduler.LambdaLR(\n","            optimizer=optimizer,\n","            lr_lambda=lr_schedule_fn\n","        )\n","\n","        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","        net = net.to(device)\n","\n","        #\n","        start_time = time.time()\n","        \n","        for epoch in range(flags.epochs):\n","            epoch_train_loss = 0.0\n","\n","            for inputs, labels in train_dataloader:\n","                inputs, labels = inputs.float().to(device), labels.to(device)\n","\n","                with torch.set_grad_enabled(True):\n","                    outputs = net(inputs)\n","                    loss = criterion(outputs, labels)\n","\n","                optimizer.zero_grad()\n","                loss.backward()\n","                optimizer.step()\n","            \n","                epoch_train_loss += loss.item()\n","            \n","            scheduler.step()\n","            \n","            wandb.log({\n","                'loss': epoch_train_loss / len(train_dataloader),\n","                'lr': optimizer.param_groups[0]['lr']\n","            })\n","\n","            if (epoch + 1) % 10 == 0:\n","                print(f'[epoch : {epoch + 1} / {flags.epochs}] Train Loss : {epoch_train_loss / len(train_dataloader)}')\n","        \n","        torch.save(net.state_dict(), f'weights_{item_idx}.pth')\n","\n","        end_time = time.time()\n","\n","        print(f'Train Time: {end_time - start_time}')\n","\n","        # Test\n","        result_np = np.zeros((1, 28), dtype=np.float32)\n","        for set_num in range(10): # 10으로 수정하기\n","            data_path = f'./test/set_{set_num}/test_{item_idx}.csv'\n","            test_dataset = testDataset(data_path)\n","\n","            inputs = test_dataset[0].reshape(1, 14, 49).to(device)\n","\n","            with torch.no_grad():\n","                outputs = net(inputs)\n","\n","            output_np = outputs.cpu().detach().numpy()\n","\n","            result_np = np.concatenate([result_np, output_np], axis=0)\n","\n","            save_df = pd.DataFrame(result_np).T\n","            save_df.to_csv(f'./set_{set_num}/predict_{item_idx}.csv', index=False)\n","            print(f'Save Result set: {set_num}, item: {item_idx}')\n","\n","flags = {\n","    'epochs': 200,\n","    'lr': 1e-4,\n","    'batch_size': 64,\n","    'data_num': 0,\n","    'model': {\n","        'nlayers': 10,\n","        'hid_dim': 1024,\n","        'weight_init': 'xavier_uniform'\n","    },\n","    'optim': 'Adam',\n","    'criterion': 'L1Loss',\n","    'dropout': 0.5,\n","    'lr_scheduler': {\n","        'method': 'Lambda'\n","    }\n","}\n","\n","train_and_test(flags)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"316osUg9zV1A","executionInfo":{"status":"ok","timestamp":1662482485840,"user_tz":-540,"elapsed":1441044,"user":{"displayName":"김성찬","userId":"03517507033102912155"}},"outputId":"31decdce-0d72-4e0e-8e56-11b61f6a16d6"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stderr","text":["ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdeep-overflow\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.13.2"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/drive/MyDrive/data/aT/scaled_data/wandb/run-20220906_161727-o69r6x58</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href=\"https://wandb.ai/deep-overflow/Nong_Linear/runs/o69r6x58\" target=\"_blank\">Test_001</a></strong> to <a href=\"https://wandb.ai/deep-overflow/Nong_Linear\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["[epoch : 10 / 200] Train Loss : 1241.9858716881793\n","[epoch : 20 / 200] Train Loss : 1211.7211064877717\n","[epoch : 30 / 200] Train Loss : 1200.4775974439538\n","[epoch : 40 / 200] Train Loss : 1179.3368981402853\n","[epoch : 50 / 200] Train Loss : 1174.688333262568\n","[epoch : 60 / 200] Train Loss : 1168.7629076086957\n","[epoch : 70 / 200] Train Loss : 1166.357278575068\n","[epoch : 80 / 200] Train Loss : 1150.6319527004075\n","[epoch : 90 / 200] Train Loss : 1161.1629585597825\n","[epoch : 100 / 200] Train Loss : 1151.306736158288\n","[epoch : 110 / 200] Train Loss : 1150.4428976307745\n","[epoch : 120 / 200] Train Loss : 1146.6574813179348\n","[epoch : 130 / 200] Train Loss : 1141.5489183508832\n","[epoch : 140 / 200] Train Loss : 1136.6665782099185\n","[epoch : 150 / 200] Train Loss : 1144.7351923403533\n","[epoch : 160 / 200] Train Loss : 1138.4159466287365\n","[epoch : 170 / 200] Train Loss : 1138.2648023522418\n","[epoch : 180 / 200] Train Loss : 1137.1538988196332\n","[epoch : 190 / 200] Train Loss : 1124.4561608355978\n","[epoch : 200 / 200] Train Loss : 1120.124779742697\n","Train Time: 36.32940196990967\n","Save Result set: 0, item: 0\n","Save Result set: 1, item: 0\n","Save Result set: 2, item: 0\n","Save Result set: 3, item: 0\n","Save Result set: 4, item: 0\n","Save Result set: 5, item: 0\n","Save Result set: 6, item: 0\n","Save Result set: 7, item: 0\n","Save Result set: 8, item: 0\n","Save Result set: 9, item: 0\n","[epoch : 10 / 200] Train Loss : 415.37733260444975\n","[epoch : 20 / 200] Train Loss : 397.7486386506454\n","[epoch : 30 / 200] Train Loss : 388.38962190047556\n","[epoch : 40 / 200] Train Loss : 377.5140977942425\n","[epoch : 50 / 200] Train Loss : 373.0705221424932\n","[epoch : 60 / 200] Train Loss : 365.4888690450917\n","[epoch : 70 / 200] Train Loss : 358.3428490680197\n","[epoch : 80 / 200] Train Loss : 356.6551686162534\n","[epoch : 90 / 200] Train Loss : 353.1392649774966\n","[epoch : 100 / 200] Train Loss : 357.39791074006456\n","[epoch : 110 / 200] Train Loss : 360.41543048361075\n","[epoch : 120 / 200] Train Loss : 349.16948401409644\n","[epoch : 130 / 200] Train Loss : 351.8209533691406\n","[epoch : 140 / 200] Train Loss : 347.32221255095106\n","[epoch : 150 / 200] Train Loss : 344.87269525942594\n","[epoch : 160 / 200] Train Loss : 335.8671078889266\n","[epoch : 170 / 200] Train Loss : 343.23151696246606\n","[epoch : 180 / 200] Train Loss : 341.3759327764096\n","[epoch : 190 / 200] Train Loss : 340.03690636676293\n","[epoch : 200 / 200] Train Loss : 340.4894435716712\n","Train Time: 34.963276624679565\n","Save Result set: 0, item: 1\n","Save Result set: 1, item: 1\n","Save Result set: 2, item: 1\n","Save Result set: 3, item: 1\n","Save Result set: 4, item: 1\n","Save Result set: 5, item: 1\n","Save Result set: 6, item: 1\n","Save Result set: 7, item: 1\n","Save Result set: 8, item: 1\n","Save Result set: 9, item: 1\n","[epoch : 10 / 200] Train Loss : 634.2854879628057\n","[epoch : 20 / 200] Train Loss : 602.7235532014266\n","[epoch : 30 / 200] Train Loss : 582.0090252420176\n","[epoch : 40 / 200] Train Loss : 572.8554979407269\n","[epoch : 50 / 200] Train Loss : 572.294005020805\n","[epoch : 60 / 200] Train Loss : 566.490782364555\n","[epoch : 70 / 200] Train Loss : 564.3464567764946\n","[epoch : 80 / 200] Train Loss : 557.0767875339674\n","[epoch : 90 / 200] Train Loss : 548.3944383704144\n","[epoch : 100 / 200] Train Loss : 562.5936876379925\n","[epoch : 110 / 200] Train Loss : 546.0450253693954\n","[epoch : 120 / 200] Train Loss : 544.803502621858\n","[epoch : 130 / 200] Train Loss : 553.4886394998301\n","[epoch : 140 / 200] Train Loss : 546.4989677097486\n","[epoch : 150 / 200] Train Loss : 540.6885548467221\n","[epoch : 160 / 200] Train Loss : 532.0352584175441\n","[epoch : 170 / 200] Train Loss : 529.5859905740489\n","[epoch : 180 / 200] Train Loss : 531.5895199983016\n","[epoch : 190 / 200] Train Loss : 521.3156990382982\n","[epoch : 200 / 200] Train Loss : 520.4369612984035\n","Train Time: 33.31717371940613\n","Save Result set: 0, item: 2\n","Save Result set: 1, item: 2\n","Save Result set: 2, item: 2\n","Save Result set: 3, item: 2\n","Save Result set: 4, item: 2\n","Save Result set: 5, item: 2\n","Save Result set: 6, item: 2\n","Save Result set: 7, item: 2\n","Save Result set: 8, item: 2\n","Save Result set: 9, item: 2\n","[epoch : 10 / 200] Train Loss : 686.1223940641984\n","[epoch : 20 / 200] Train Loss : 658.8742649244225\n","[epoch : 30 / 200] Train Loss : 633.111083984375\n","[epoch : 40 / 200] Train Loss : 618.6186417289402\n","[epoch : 50 / 200] Train Loss : 635.3313784391984\n","[epoch : 60 / 200] Train Loss : 618.8953167459239\n","[epoch : 70 / 200] Train Loss : 618.5518400772759\n","[epoch : 80 / 200] Train Loss : 613.5574871560801\n","[epoch : 90 / 200] Train Loss : 612.6378784179688\n","[epoch : 100 / 200] Train Loss : 606.6158075747283\n","[epoch : 110 / 200] Train Loss : 603.0343601392663\n","[epoch : 120 / 200] Train Loss : 599.3914317255435\n","[epoch : 130 / 200] Train Loss : 600.5046917459239\n","[epoch : 140 / 200] Train Loss : 596.7356116253396\n","[epoch : 150 / 200] Train Loss : 592.8777147376019\n","[epoch : 160 / 200] Train Loss : 589.4894621475884\n","[epoch : 170 / 200] Train Loss : 586.2873495350714\n","[epoch : 180 / 200] Train Loss : 580.1404551630435\n","[epoch : 190 / 200] Train Loss : 579.111012334409\n","[epoch : 200 / 200] Train Loss : 580.9403898819634\n","Train Time: 33.46429133415222\n","Save Result set: 0, item: 3\n","Save Result set: 1, item: 3\n","Save Result set: 2, item: 3\n","Save Result set: 3, item: 3\n","Save Result set: 4, item: 3\n","Save Result set: 5, item: 3\n","Save Result set: 6, item: 3\n","Save Result set: 7, item: 3\n","Save Result set: 8, item: 3\n","Save Result set: 9, item: 3\n","[epoch : 10 / 200] Train Loss : 639.431372601053\n","[epoch : 20 / 200] Train Loss : 591.7941257642663\n","[epoch : 30 / 200] Train Loss : 573.9451134723166\n","[epoch : 40 / 200] Train Loss : 571.8792472507643\n","[epoch : 50 / 200] Train Loss : 567.4810791015625\n","[epoch : 60 / 200] Train Loss : 563.5852369225544\n","[epoch : 70 / 200] Train Loss : 549.1039309294327\n","[epoch : 80 / 200] Train Loss : 542.585252844769\n","[epoch : 90 / 200] Train Loss : 541.9343182107676\n","[epoch : 100 / 200] Train Loss : 555.2070657481318\n","[epoch : 110 / 200] Train Loss : 549.7759930154551\n","[epoch : 120 / 200] Train Loss : 546.5179509701936\n","[epoch : 130 / 200] Train Loss : 536.2450163468071\n","[epoch : 140 / 200] Train Loss : 527.3375310483186\n","[epoch : 150 / 200] Train Loss : 532.5728056534477\n","[epoch : 160 / 200] Train Loss : 528.0003144637398\n","[epoch : 170 / 200] Train Loss : 528.6624888544497\n","[epoch : 180 / 200] Train Loss : 521.5274897036345\n","[epoch : 190 / 200] Train Loss : 526.3440976350204\n","[epoch : 200 / 200] Train Loss : 530.311731753142\n","Train Time: 37.50217342376709\n","Save Result set: 0, item: 4\n","Save Result set: 1, item: 4\n","Save Result set: 2, item: 4\n","Save Result set: 3, item: 4\n","Save Result set: 4, item: 4\n","Save Result set: 5, item: 4\n","Save Result set: 6, item: 4\n","Save Result set: 7, item: 4\n","Save Result set: 8, item: 4\n","Save Result set: 9, item: 4\n","[epoch : 10 / 200] Train Loss : 1309.3028139860733\n","[epoch : 20 / 200] Train Loss : 1249.3005477241848\n","[epoch : 30 / 200] Train Loss : 1224.244241465693\n","[epoch : 40 / 200] Train Loss : 1246.5709706182065\n","[epoch : 50 / 200] Train Loss : 1187.9082615064538\n","[epoch : 60 / 200] Train Loss : 1179.8886506453805\n","[epoch : 70 / 200] Train Loss : 1177.459907863451\n","[epoch : 80 / 200] Train Loss : 1169.166408372962\n","[epoch : 90 / 200] Train Loss : 1160.028123938519\n","[epoch : 100 / 200] Train Loss : 1186.8219312584918\n","[epoch : 110 / 200] Train Loss : 1153.7812393851902\n","[epoch : 120 / 200] Train Loss : 1152.3892105765965\n","[epoch : 130 / 200] Train Loss : 1138.973484205163\n","[epoch : 140 / 200] Train Loss : 1157.7585661514945\n","[epoch : 150 / 200] Train Loss : 1156.1826171875\n","[epoch : 160 / 200] Train Loss : 1147.6494034476902\n","[epoch : 170 / 200] Train Loss : 1150.5297108525815\n","[epoch : 180 / 200] Train Loss : 1143.4164136803668\n","[epoch : 190 / 200] Train Loss : 1124.3200418223505\n","[epoch : 200 / 200] Train Loss : 1145.060594641644\n","Train Time: 33.6348021030426\n","Save Result set: 0, item: 5\n","Save Result set: 1, item: 5\n","Save Result set: 2, item: 5\n","Save Result set: 3, item: 5\n","Save Result set: 4, item: 5\n","Save Result set: 5, item: 5\n","Save Result set: 6, item: 5\n","Save Result set: 7, item: 5\n","Save Result set: 8, item: 5\n","Save Result set: 9, item: 5\n","[epoch : 10 / 200] Train Loss : 2253.8055048403535\n","[epoch : 20 / 200] Train Loss : 2189.4404509171195\n","[epoch : 30 / 200] Train Loss : 2176.451368248981\n","[epoch : 40 / 200] Train Loss : 2171.563221807065\n","[epoch : 50 / 200] Train Loss : 2178.719132133152\n","[epoch : 60 / 200] Train Loss : 2136.614953082541\n","[epoch : 70 / 200] Train Loss : 2219.384043817935\n","[epoch : 80 / 200] Train Loss : 2165.85643469769\n","[epoch : 90 / 200] Train Loss : 2164.340183423913\n","[epoch : 100 / 200] Train Loss : 2129.902587890625\n","[epoch : 110 / 200] Train Loss : 2132.448688009511\n","[epoch : 120 / 200] Train Loss : 2164.26098102072\n","[epoch : 130 / 200] Train Loss : 2186.627000891644\n","[epoch : 140 / 200] Train Loss : 2130.180966584579\n","[epoch : 150 / 200] Train Loss : 2152.980272376019\n","[epoch : 160 / 200] Train Loss : 2129.117962381114\n","[epoch : 170 / 200] Train Loss : 2121.423196543818\n","[epoch : 180 / 200] Train Loss : 2176.313375721807\n","[epoch : 190 / 200] Train Loss : 2107.5632961107335\n","[epoch : 200 / 200] Train Loss : 2135.6318996263585\n","Train Time: 35.621567726135254\n","Save Result set: 0, item: 6\n","Save Result set: 1, item: 6\n","Save Result set: 2, item: 6\n","Save Result set: 3, item: 6\n","Save Result set: 4, item: 6\n","Save Result set: 5, item: 6\n","Save Result set: 6, item: 6\n","Save Result set: 7, item: 6\n","Save Result set: 8, item: 6\n","Save Result set: 9, item: 6\n","[epoch : 10 / 200] Train Loss : 734.3316252335259\n","[epoch : 20 / 200] Train Loss : 721.1461606233016\n","[epoch : 30 / 200] Train Loss : 706.858666461447\n","[epoch : 40 / 200] Train Loss : 689.9520449431046\n","[epoch : 50 / 200] Train Loss : 681.2531154466712\n","[epoch : 60 / 200] Train Loss : 689.572801672894\n","[epoch : 70 / 200] Train Loss : 674.388647991678\n","[epoch : 80 / 200] Train Loss : 678.3295818826426\n","[epoch : 90 / 200] Train Loss : 691.4199908712636\n","[epoch : 100 / 200] Train Loss : 673.1737670898438\n","[epoch : 110 / 200] Train Loss : 674.7535957668139\n","[epoch : 120 / 200] Train Loss : 679.582591181216\n","[epoch : 130 / 200] Train Loss : 666.7995791227921\n","[epoch : 140 / 200] Train Loss : 666.5266007133152\n","[epoch : 150 / 200] Train Loss : 664.1009176503057\n","[epoch : 160 / 200] Train Loss : 657.61877176036\n","[epoch : 170 / 200] Train Loss : 659.9383332625679\n","[epoch : 180 / 200] Train Loss : 668.7013496730639\n","[epoch : 190 / 200] Train Loss : 657.4880716075068\n","[epoch : 200 / 200] Train Loss : 661.3106742527174\n","Train Time: 33.38858199119568\n","Save Result set: 0, item: 7\n","Save Result set: 1, item: 7\n","Save Result set: 2, item: 7\n","Save Result set: 3, item: 7\n","Save Result set: 4, item: 7\n","Save Result set: 5, item: 7\n","Save Result set: 6, item: 7\n","Save Result set: 7, item: 7\n","Save Result set: 8, item: 7\n","no data in Dataset!!\n","Empty DataFrame\n","Columns: [Unnamed: 0, 단가(원), 거래량, 거래대금(원), 경매건수, 도매시장코드, 도매법인코드, 산지코드 , 해당일자_전체평균가격(원), 해당일자_전체거래물량(kg), 하위가격 평균가(원), 상위가격 평균가(원), 하위가격 거래물량(kg), 상위가격 거래물량(kg), 일자별_도매가격_최대(원), 일자별_도매가격_평균(원), 일자별_도매가격_최소(원), datadate, 일자별_소매가격_최대(원), 일자별_소매가격_평균(원), 일자별_소매가격_최소(원), 수출중량(kg), 수출금액(달러), 수입중량(kg), 수입금액(달러), 무역수지(달러), 주산지_0_초기온도(℃), 주산지_0_최대온도(℃), 주산지_0_최저온도(℃), 주산지_0_평균온도(℃), 주산지_0_강수량(ml), 주산지_0_습도(%), 주산지_1_초기온도(℃), 주산지_1_최대온도(℃), 주산지_1_최저온도(℃), 주산지_1_평균온도(℃), 주산지_1_강수량(ml), 주산지_1_습도(%), 주산지_2_초기온도(℃), 주산지_2_최대온도(℃), 주산지_2_최저온도(℃), 주산지_2_평균온도(℃), 주산지_2_강수량(ml), 주산지_2_습도(%)]\n","Index: []\n","\n","[0 rows x 44 columns]\n","   Unnamed: 0 단가(원)  거래량 거래대금(원) 경매건수 도매시장코드 도매법인코드 산지코드  해당일자_전체평균가격(원)  \\\n","0         NaN   NaN  NaN     NaN  NaN    NaN    NaN   NaN            NaN   \n","1         NaN   NaN  NaN     NaN  NaN    NaN    NaN   NaN            NaN   \n","2         NaN   NaN  NaN     NaN  NaN    NaN    NaN   NaN            NaN   \n","3         NaN   NaN  NaN     NaN  NaN    NaN    NaN   NaN            NaN   \n","4         NaN   NaN  NaN     NaN  NaN    NaN    NaN   NaN            NaN   \n","5         NaN   NaN  NaN     NaN  NaN    NaN    NaN   NaN            NaN   \n","6         NaN   NaN  NaN     NaN  NaN    NaN    NaN   NaN            NaN   \n","7         NaN   NaN  NaN     NaN  NaN    NaN    NaN   NaN            NaN   \n","8         NaN   NaN  NaN     NaN  NaN    NaN    NaN   NaN            NaN   \n","9         NaN   NaN  NaN     NaN  NaN    NaN    NaN   NaN            NaN   \n","10        NaN   NaN  NaN     NaN  NaN    NaN    NaN   NaN            NaN   \n","11        NaN   NaN  NaN     NaN  NaN    NaN    NaN   NaN            NaN   \n","12        NaN   NaN  NaN     NaN  NaN    NaN    NaN   NaN            NaN   \n","13        NaN   NaN  NaN     NaN  NaN    NaN    NaN   NaN            NaN   \n","\n","   해당일자_전체거래물량(kg)  ... 주산지_1_평균온도(℃) 주산지_1_강수량(ml) 주산지_1_습도(%) 주산지_2_초기온도(℃)  \\\n","0              NaN  ...           NaN           NaN         NaN           NaN   \n","1              NaN  ...           NaN           NaN         NaN           NaN   \n","2              NaN  ...           NaN           NaN         NaN           NaN   \n","3              NaN  ...           NaN           NaN         NaN           NaN   \n","4              NaN  ...           NaN           NaN         NaN           NaN   \n","5              NaN  ...           NaN           NaN         NaN           NaN   \n","6              NaN  ...           NaN           NaN         NaN           NaN   \n","7              NaN  ...           NaN           NaN         NaN           NaN   \n","8              NaN  ...           NaN           NaN         NaN           NaN   \n","9              NaN  ...           NaN           NaN         NaN           NaN   \n","10             NaN  ...           NaN           NaN         NaN           NaN   \n","11             NaN  ...           NaN           NaN         NaN           NaN   \n","12             NaN  ...           NaN           NaN         NaN           NaN   \n","13             NaN  ...           NaN           NaN         NaN           NaN   \n","\n","   주산지_2_최대온도(℃) 주산지_2_최저온도(℃) 주산지_2_평균온도(℃) 주산지_2_강수량(ml) 주산지_2_습도(%)  \\\n","0            NaN           NaN           NaN           NaN         NaN   \n","1            NaN           NaN           NaN           NaN         NaN   \n","2            NaN           NaN           NaN           NaN         NaN   \n","3            NaN           NaN           NaN           NaN         NaN   \n","4            NaN           NaN           NaN           NaN         NaN   \n","5            NaN           NaN           NaN           NaN         NaN   \n","6            NaN           NaN           NaN           NaN         NaN   \n","7            NaN           NaN           NaN           NaN         NaN   \n","8            NaN           NaN           NaN           NaN         NaN   \n","9            NaN           NaN           NaN           NaN         NaN   \n","10           NaN           NaN           NaN           NaN         NaN   \n","11           NaN           NaN           NaN           NaN         NaN   \n","12           NaN           NaN           NaN           NaN         NaN   \n","13           NaN           NaN           NaN           NaN         NaN   \n","\n","   zero_non  \n","0         0  \n","1         0  \n","2         0  \n","3         0  \n","4         0  \n","5         0  \n","6         0  \n","7         0  \n","8         0  \n","9         0  \n","10        0  \n","11        0  \n","12        0  \n","13        0  \n","\n","[14 rows x 45 columns]\n","    Unnamed: 0  단가(원)  거래량  거래대금(원)  경매건수  도매시장코드  도매법인코드  산지코드   \\\n","0            0      0    0        0     0       0       0      0   \n","1            0      0    0        0     0       0       0      0   \n","2            0      0    0        0     0       0       0      0   \n","3            0      0    0        0     0       0       0      0   \n","4            0      0    0        0     0       0       0      0   \n","5            0      0    0        0     0       0       0      0   \n","6            0      0    0        0     0       0       0      0   \n","7            0      0    0        0     0       0       0      0   \n","8            0      0    0        0     0       0       0      0   \n","9            0      0    0        0     0       0       0      0   \n","10           0      0    0        0     0       0       0      0   \n","11           0      0    0        0     0       0       0      0   \n","12           0      0    0        0     0       0       0      0   \n","13           0      0    0        0     0       0       0      0   \n","\n","    해당일자_전체평균가격(원)  해당일자_전체거래물량(kg)  ...  주산지_1_평균온도(℃)  주산지_1_강수량(ml)  \\\n","0                0                0  ...              0              0   \n","1                0                0  ...              0              0   \n","2                0                0  ...              0              0   \n","3                0                0  ...              0              0   \n","4                0                0  ...              0              0   \n","5                0                0  ...              0              0   \n","6                0                0  ...              0              0   \n","7                0                0  ...              0              0   \n","8                0                0  ...              0              0   \n","9                0                0  ...              0              0   \n","10               0                0  ...              0              0   \n","11               0                0  ...              0              0   \n","12               0                0  ...              0              0   \n","13               0                0  ...              0              0   \n","\n","    주산지_1_습도(%)  주산지_2_초기온도(℃)  주산지_2_최대온도(℃)  주산지_2_최저온도(℃)  주산지_2_평균온도(℃)  \\\n","0             0              0              0              0              0   \n","1             0              0              0              0              0   \n","2             0              0              0              0              0   \n","3             0              0              0              0              0   \n","4             0              0              0              0              0   \n","5             0              0              0              0              0   \n","6             0              0              0              0              0   \n","7             0              0              0              0              0   \n","8             0              0              0              0              0   \n","9             0              0              0              0              0   \n","10            0              0              0              0              0   \n","11            0              0              0              0              0   \n","12            0              0              0              0              0   \n","13            0              0              0              0              0   \n","\n","    주산지_2_강수량(ml)  주산지_2_습도(%)  zero_non  \n","0               0            0         0  \n","1               0            0         0  \n","2               0            0         0  \n","3               0            0         0  \n","4               0            0         0  \n","5               0            0         0  \n","6               0            0         0  \n","7               0            0         0  \n","8               0            0         0  \n","9               0            0         0  \n","10              0            0         0  \n","11              0            0         0  \n","12              0            0         0  \n","13              0            0         0  \n","\n","[14 rows x 45 columns]\n","    단가(원)  거래량  거래대금(원)  경매건수  도매시장코드  도매법인코드  산지코드   해당일자_전체평균가격(원)  \\\n","0       0    0        0     0       0       0      0               0   \n","1       0    0        0     0       0       0      0               0   \n","2       0    0        0     0       0       0      0               0   \n","3       0    0        0     0       0       0      0               0   \n","4       0    0        0     0       0       0      0               0   \n","5       0    0        0     0       0       0      0               0   \n","6       0    0        0     0       0       0      0               0   \n","7       0    0        0     0       0       0      0               0   \n","8       0    0        0     0       0       0      0               0   \n","9       0    0        0     0       0       0      0               0   \n","10      0    0        0     0       0       0      0               0   \n","11      0    0        0     0       0       0      0               0   \n","12      0    0        0     0       0       0      0               0   \n","13      0    0        0     0       0       0      0               0   \n","\n","    해당일자_전체거래물량(kg)  하위가격 평균가(원)  ...  주산지_1_최저온도(℃)  주산지_1_평균온도(℃)  \\\n","0                 0            0  ...              0              0   \n","1                 0            0  ...              0              0   \n","2                 0            0  ...              0              0   \n","3                 0            0  ...              0              0   \n","4                 0            0  ...              0              0   \n","5                 0            0  ...              0              0   \n","6                 0            0  ...              0              0   \n","7                 0            0  ...              0              0   \n","8                 0            0  ...              0              0   \n","9                 0            0  ...              0              0   \n","10                0            0  ...              0              0   \n","11                0            0  ...              0              0   \n","12                0            0  ...              0              0   \n","13                0            0  ...              0              0   \n","\n","    주산지_1_강수량(ml)  주산지_1_습도(%)  주산지_2_초기온도(℃)  주산지_2_최대온도(℃)  주산지_2_최저온도(℃)  \\\n","0               0            0              0              0              0   \n","1               0            0              0              0              0   \n","2               0            0              0              0              0   \n","3               0            0              0              0              0   \n","4               0            0              0              0              0   \n","5               0            0              0              0              0   \n","6               0            0              0              0              0   \n","7               0            0              0              0              0   \n","8               0            0              0              0              0   \n","9               0            0              0              0              0   \n","10              0            0              0              0              0   \n","11              0            0              0              0              0   \n","12              0            0              0              0              0   \n","13              0            0              0              0              0   \n","\n","    주산지_2_평균온도(℃)  주산지_2_강수량(ml)  주산지_2_습도(%)  \n","0               0              0            0  \n","1               0              0            0  \n","2               0              0            0  \n","3               0              0            0  \n","4               0              0            0  \n","5               0              0            0  \n","6               0              0            0  \n","7               0              0            0  \n","8               0              0            0  \n","9               0              0            0  \n","10              0              0            0  \n","11              0              0            0  \n","12              0              0            0  \n","13              0              0            0  \n","\n","[14 rows x 43 columns]\n","Save Result set: 9, item: 7\n","[epoch : 10 / 200] Train Loss : 1155.2190206776495\n","[epoch : 20 / 200] Train Loss : 1108.1361667798913\n","[epoch : 30 / 200] Train Loss : 1082.0568741508152\n","[epoch : 40 / 200] Train Loss : 1069.203321373981\n","[epoch : 50 / 200] Train Loss : 1069.6845968495245\n","[epoch : 60 / 200] Train Loss : 1054.3427150560462\n","[epoch : 70 / 200] Train Loss : 1051.1844349736753\n","[epoch : 80 / 200] Train Loss : 1057.594601838485\n","[epoch : 90 / 200] Train Loss : 1036.3401728091033\n","[epoch : 100 / 200] Train Loss : 1037.6210725203805\n","[epoch : 110 / 200] Train Loss : 1025.3574112601902\n","[epoch : 120 / 200] Train Loss : 1029.0534455672555\n","[epoch : 130 / 200] Train Loss : 1021.9658203125\n","[epoch : 140 / 200] Train Loss : 1024.6888613493545\n","[epoch : 150 / 200] Train Loss : 1028.8286530867867\n","[epoch : 160 / 200] Train Loss : 1023.0955704398777\n","[epoch : 170 / 200] Train Loss : 1010.0158983313519\n","[epoch : 180 / 200] Train Loss : 1007.8113589079484\n","[epoch : 190 / 200] Train Loss : 1000.15661886464\n","[epoch : 200 / 200] Train Loss : 1006.1702535878057\n","Train Time: 35.173802852630615\n","Save Result set: 0, item: 8\n","Save Result set: 1, item: 8\n","Save Result set: 2, item: 8\n","Save Result set: 3, item: 8\n","Save Result set: 4, item: 8\n","Save Result set: 5, item: 8\n","Save Result set: 6, item: 8\n","Save Result set: 7, item: 8\n","Save Result set: 8, item: 8\n","Save Result set: 9, item: 8\n","[epoch : 10 / 200] Train Loss : 474.4832100246264\n","[epoch : 20 / 200] Train Loss : 428.548107644786\n","[epoch : 30 / 200] Train Loss : 424.10650236710256\n","[epoch : 40 / 200] Train Loss : 414.7034978451936\n","[epoch : 50 / 200] Train Loss : 416.7224545686141\n","[epoch : 60 / 200] Train Loss : 411.9379537831182\n","[epoch : 70 / 200] Train Loss : 406.1717529296875\n","[epoch : 80 / 200] Train Loss : 405.2075872006624\n","[epoch : 90 / 200] Train Loss : 403.9252823539402\n","[epoch : 100 / 200] Train Loss : 399.75885805876356\n","[epoch : 110 / 200] Train Loss : 393.7103457243546\n","[epoch : 120 / 200] Train Loss : 399.7309105914572\n","[epoch : 130 / 200] Train Loss : 392.00843744692594\n","[epoch : 140 / 200] Train Loss : 397.089087444803\n","[epoch : 150 / 200] Train Loss : 393.2999108355978\n","[epoch : 160 / 200] Train Loss : 398.0856018066406\n","[epoch : 170 / 200] Train Loss : 382.76341844641644\n","[epoch : 180 / 200] Train Loss : 385.24202429729957\n","[epoch : 190 / 200] Train Loss : 379.1042706033458\n","[epoch : 200 / 200] Train Loss : 373.98800393809444\n","Train Time: 33.69146156311035\n","Save Result set: 0, item: 9\n","Save Result set: 1, item: 9\n","Save Result set: 2, item: 9\n","Save Result set: 3, item: 9\n","Save Result set: 4, item: 9\n","Save Result set: 5, item: 9\n","Save Result set: 6, item: 9\n","Save Result set: 7, item: 9\n","Save Result set: 8, item: 9\n","Save Result set: 9, item: 9\n","[epoch : 10 / 200] Train Loss : 495.6056107230808\n","[epoch : 20 / 200] Train Loss : 475.60285485309106\n","[epoch : 30 / 200] Train Loss : 457.41804039996606\n","[epoch : 40 / 200] Train Loss : 452.7721026876698\n","[epoch : 50 / 200] Train Loss : 449.51044895337975\n","[epoch : 60 / 200] Train Loss : 455.6157903256624\n","[epoch : 70 / 200] Train Loss : 446.5959990128227\n","[epoch : 80 / 200] Train Loss : 440.254638671875\n","[epoch : 90 / 200] Train Loss : 447.8331882642663\n","[epoch : 100 / 200] Train Loss : 434.6936234183933\n","[epoch : 110 / 200] Train Loss : 434.6083692467731\n","[epoch : 120 / 200] Train Loss : 429.64869623598844\n","[epoch : 130 / 200] Train Loss : 428.142613949983\n","[epoch : 140 / 200] Train Loss : 425.91977857506794\n","[epoch : 150 / 200] Train Loss : 426.5867800505265\n","[epoch : 160 / 200] Train Loss : 425.30656366762906\n","[epoch : 170 / 200] Train Loss : 420.06004399838656\n","[epoch : 180 / 200] Train Loss : 414.90613058338994\n","[epoch : 190 / 200] Train Loss : 420.36008619225544\n","[epoch : 200 / 200] Train Loss : 409.97426970108694\n","Train Time: 34.01410627365112\n","Save Result set: 0, item: 10\n","Save Result set: 1, item: 10\n","Save Result set: 2, item: 10\n","Save Result set: 3, item: 10\n","Save Result set: 4, item: 10\n","Save Result set: 5, item: 10\n","Save Result set: 6, item: 10\n","Save Result set: 7, item: 10\n","Save Result set: 8, item: 10\n","Save Result set: 9, item: 10\n","[epoch : 10 / 200] Train Loss : 1159.0083803923233\n","[epoch : 20 / 200] Train Loss : 1131.7455789317255\n","[epoch : 30 / 200] Train Loss : 1095.1261649753737\n","[epoch : 40 / 200] Train Loss : 1082.884128736413\n","[epoch : 50 / 200] Train Loss : 1068.4804077148438\n","[epoch : 60 / 200] Train Loss : 1060.6451044497283\n","[epoch : 70 / 200] Train Loss : 1066.361917246943\n","[epoch : 80 / 200] Train Loss : 1056.4322403617527\n","[epoch : 90 / 200] Train Loss : 1052.381668754246\n","[epoch : 100 / 200] Train Loss : 1029.9426004161005\n","[epoch : 110 / 200] Train Loss : 1033.3867346722147\n","[epoch : 120 / 200] Train Loss : 1027.3808195694633\n","[epoch : 130 / 200] Train Loss : 1035.738626231318\n","[epoch : 140 / 200] Train Loss : 1017.1802766219429\n","[epoch : 150 / 200] Train Loss : 1029.2036292034647\n","[epoch : 160 / 200] Train Loss : 1022.5281106700068\n","[epoch : 170 / 200] Train Loss : 1020.9843856148098\n","[epoch : 180 / 200] Train Loss : 1022.9827509341033\n","[epoch : 190 / 200] Train Loss : 1009.9849455460259\n","[epoch : 200 / 200] Train Loss : 1019.8613944675611\n","Train Time: 34.100168228149414\n","Save Result set: 0, item: 11\n","Save Result set: 1, item: 11\n","Save Result set: 2, item: 11\n","Save Result set: 3, item: 11\n","Save Result set: 4, item: 11\n","Save Result set: 5, item: 11\n","Save Result set: 6, item: 11\n","Save Result set: 7, item: 11\n","Save Result set: 8, item: 11\n","Save Result set: 9, item: 11\n","[epoch : 10 / 200] Train Loss : 952.9076086956521\n","[epoch : 20 / 200] Train Loss : 909.258348547894\n","[epoch : 30 / 200] Train Loss : 898.0767371136209\n","[epoch : 40 / 200] Train Loss : 890.5892015540081\n","[epoch : 50 / 200] Train Loss : 881.6028017790421\n","[epoch : 60 / 200] Train Loss : 866.9534195609715\n","[epoch : 70 / 200] Train Loss : 866.9619167162025\n","[epoch : 80 / 200] Train Loss : 838.9929836107337\n","[epoch : 90 / 200] Train Loss : 835.4598972486413\n","[epoch : 100 / 200] Train Loss : 851.038133704144\n","[epoch : 110 / 200] Train Loss : 840.626560377038\n","[epoch : 120 / 200] Train Loss : 846.8262010657269\n","[epoch : 130 / 200] Train Loss : 824.6334148904551\n","[epoch : 140 / 200] Train Loss : 829.5888990319294\n","[epoch : 150 / 200] Train Loss : 820.7689394743546\n","[epoch : 160 / 200] Train Loss : 837.9220899498981\n","[epoch : 170 / 200] Train Loss : 832.2898134977921\n","[epoch : 180 / 200] Train Loss : 813.7557956861413\n","[epoch : 190 / 200] Train Loss : 806.0113313094429\n","[epoch : 200 / 200] Train Loss : 806.6848967179009\n","Train Time: 33.365132570266724\n","Save Result set: 0, item: 12\n","Save Result set: 1, item: 12\n","Save Result set: 2, item: 12\n","Save Result set: 3, item: 12\n","Save Result set: 4, item: 12\n","Save Result set: 5, item: 12\n","Save Result set: 6, item: 12\n","Save Result set: 7, item: 12\n","Save Result set: 8, item: 12\n","Save Result set: 9, item: 12\n","[epoch : 10 / 200] Train Loss : 780.0121804942255\n","[epoch : 20 / 200] Train Loss : 739.1998131793479\n","[epoch : 30 / 200] Train Loss : 721.5812377929688\n","[epoch : 40 / 200] Train Loss : 708.9464668605639\n","[epoch : 50 / 200] Train Loss : 697.6346143639606\n","[epoch : 60 / 200] Train Loss : 697.333859650985\n","[epoch : 70 / 200] Train Loss : 698.591552734375\n","[epoch : 80 / 200] Train Loss : 691.7948900305706\n","[epoch : 90 / 200] Train Loss : 684.5246290123981\n","[epoch : 100 / 200] Train Loss : 678.4927208941916\n","[epoch : 110 / 200] Train Loss : 670.6028946586277\n","[epoch : 120 / 200] Train Loss : 676.4248338782269\n","[epoch : 130 / 200] Train Loss : 662.9563810929009\n","[epoch : 140 / 200] Train Loss : 666.2404227878736\n","[epoch : 150 / 200] Train Loss : 666.9575460682745\n","[epoch : 160 / 200] Train Loss : 654.0979455035666\n","[epoch : 170 / 200] Train Loss : 665.1446161684783\n","[epoch : 180 / 200] Train Loss : 655.3619968580163\n","[epoch : 190 / 200] Train Loss : 652.7631278659986\n","[epoch : 200 / 200] Train Loss : 640.0563301418139\n","Train Time: 33.48133301734924\n","Save Result set: 0, item: 13\n","Save Result set: 1, item: 13\n","Save Result set: 2, item: 13\n","Save Result set: 3, item: 13\n","Save Result set: 4, item: 13\n","Save Result set: 5, item: 13\n","Save Result set: 6, item: 13\n","Save Result set: 7, item: 13\n","Save Result set: 8, item: 13\n","Save Result set: 9, item: 13\n","[epoch : 10 / 200] Train Loss : 2583.253078294837\n","[epoch : 20 / 200] Train Loss : 2511.666790506114\n","[epoch : 30 / 200] Train Loss : 2499.6031122622285\n","[epoch : 40 / 200] Train Loss : 2405.220363451087\n","[epoch : 50 / 200] Train Loss : 2393.715618631114\n","[epoch : 60 / 200] Train Loss : 2420.753025220788\n","[epoch : 70 / 200] Train Loss : 2381.4230107846465\n","[epoch : 80 / 200] Train Loss : 2390.106795601223\n","[epoch : 90 / 200] Train Loss : 2396.1868418817935\n","[epoch : 100 / 200] Train Loss : 2367.541355298913\n","[epoch : 110 / 200] Train Loss : 2354.359077785326\n","[epoch : 120 / 200] Train Loss : 2371.075534986413\n","[epoch : 130 / 200] Train Loss : 2359.0992378566575\n","[epoch : 140 / 200] Train Loss : 2335.7815684442935\n","[epoch : 150 / 200] Train Loss : 2339.877483865489\n","[epoch : 160 / 200] Train Loss : 2345.364544412364\n","[epoch : 170 / 200] Train Loss : 2390.613334324049\n","[epoch : 180 / 200] Train Loss : 2343.105670431386\n","[epoch : 190 / 200] Train Loss : 2303.6917777683425\n","[epoch : 200 / 200] Train Loss : 2336.729290506114\n","Train Time: 33.58632755279541\n","Save Result set: 0, item: 14\n","Save Result set: 1, item: 14\n","Save Result set: 2, item: 14\n","Save Result set: 3, item: 14\n","Save Result set: 4, item: 14\n","Save Result set: 5, item: 14\n","Save Result set: 6, item: 14\n","Save Result set: 7, item: 14\n","Save Result set: 8, item: 14\n","Save Result set: 9, item: 14\n","[epoch : 10 / 200] Train Loss : 582.1076076341712\n","[epoch : 20 / 200] Train Loss : 547.8638982358186\n","[epoch : 30 / 200] Train Loss : 521.0028235394021\n","[epoch : 40 / 200] Train Loss : 516.1543764860734\n","[epoch : 50 / 200] Train Loss : 515.6114501953125\n","[epoch : 60 / 200] Train Loss : 505.8755997367527\n","[epoch : 70 / 200] Train Loss : 508.57232267960256\n","[epoch : 80 / 200] Train Loss : 493.58976015837294\n","[epoch : 90 / 200] Train Loss : 493.3169409710428\n","[epoch : 100 / 200] Train Loss : 491.9239541758662\n","[epoch : 110 / 200] Train Loss : 487.60128518809444\n","[epoch : 120 / 200] Train Loss : 479.3684028957201\n","[epoch : 130 / 200] Train Loss : 478.4208307680876\n","[epoch : 140 / 200] Train Loss : 473.1357023819633\n","[epoch : 150 / 200] Train Loss : 466.3251767365829\n","[epoch : 160 / 200] Train Loss : 467.1590496560802\n","[epoch : 170 / 200] Train Loss : 463.13168733016306\n","[epoch : 180 / 200] Train Loss : 468.19942441193956\n","[epoch : 190 / 200] Train Loss : 466.0405552076257\n","[epoch : 200 / 200] Train Loss : 459.0460722550102\n","Train Time: 33.572190284729004\n","Save Result set: 0, item: 15\n","Save Result set: 1, item: 15\n","Save Result set: 2, item: 15\n","Save Result set: 3, item: 15\n","Save Result set: 4, item: 15\n","Save Result set: 5, item: 15\n","Save Result set: 6, item: 15\n","Save Result set: 7, item: 15\n","Save Result set: 8, item: 15\n","Save Result set: 9, item: 15\n","[epoch : 10 / 200] Train Loss : 1130.4456415591033\n","[epoch : 20 / 200] Train Loss : 1064.2983557659647\n","[epoch : 30 / 200] Train Loss : 1046.1837688943615\n","[epoch : 40 / 200] Train Loss : 1026.3727576214335\n","[epoch : 50 / 200] Train Loss : 1009.7797294284986\n","[epoch : 60 / 200] Train Loss : 1012.6667878524116\n","[epoch : 70 / 200] Train Loss : 1010.3238605001699\n","[epoch : 80 / 200] Train Loss : 1001.5398639181386\n","[epoch : 90 / 200] Train Loss : 981.0697711447011\n","[epoch : 100 / 200] Train Loss : 982.8607071586277\n","[epoch : 110 / 200] Train Loss : 975.1621465268342\n","[epoch : 120 / 200] Train Loss : 978.0580736243206\n","[epoch : 130 / 200] Train Loss : 975.6949807871943\n","[epoch : 140 / 200] Train Loss : 967.3374129585598\n","[epoch : 150 / 200] Train Loss : 978.9522784689199\n","[epoch : 160 / 200] Train Loss : 963.1080906080163\n","[epoch : 170 / 200] Train Loss : 965.7126624065896\n","[epoch : 180 / 200] Train Loss : 955.6991391389266\n","[epoch : 190 / 200] Train Loss : 958.38330078125\n","[epoch : 200 / 200] Train Loss : 956.206763226053\n","Train Time: 33.40199828147888\n","Save Result set: 0, item: 16\n","Save Result set: 1, item: 16\n","Save Result set: 2, item: 16\n","Save Result set: 3, item: 16\n","Save Result set: 4, item: 16\n","Save Result set: 5, item: 16\n","Save Result set: 6, item: 16\n","Save Result set: 7, item: 16\n","Save Result set: 8, item: 16\n","Save Result set: 9, item: 16\n","[epoch : 10 / 200] Train Loss : 746.4572408924932\n","[epoch : 20 / 200] Train Loss : 728.4221987516984\n","[epoch : 30 / 200] Train Loss : 702.784763502038\n","[epoch : 40 / 200] Train Loss : 693.5540108058764\n","[epoch : 50 / 200] Train Loss : 676.6538404381794\n","[epoch : 60 / 200] Train Loss : 664.77306863536\n","[epoch : 70 / 200] Train Loss : 659.3977714206861\n","[epoch : 80 / 200] Train Loss : 653.791111158288\n","[epoch : 90 / 200] Train Loss : 655.9170319930366\n","[epoch : 100 / 200] Train Loss : 650.3793122664741\n","[epoch : 110 / 200] Train Loss : 649.3929735266644\n","[epoch : 120 / 200] Train Loss : 639.0423292077106\n","[epoch : 130 / 200] Train Loss : 635.3503099524456\n","[epoch : 140 / 200] Train Loss : 648.967624830163\n","[epoch : 150 / 200] Train Loss : 632.024562669837\n","[epoch : 160 / 200] Train Loss : 637.4020677649456\n","[epoch : 170 / 200] Train Loss : 629.3445885699729\n","[epoch : 180 / 200] Train Loss : 627.4726244055706\n","[epoch : 190 / 200] Train Loss : 625.0326829993206\n","[epoch : 200 / 200] Train Loss : 631.3232368800951\n","Train Time: 34.098740100860596\n","Save Result set: 0, item: 17\n","Save Result set: 1, item: 17\n","Save Result set: 2, item: 17\n","Save Result set: 3, item: 17\n","Save Result set: 4, item: 17\n","Save Result set: 5, item: 17\n","Save Result set: 6, item: 17\n","Save Result set: 7, item: 17\n","Save Result set: 8, item: 17\n","Save Result set: 9, item: 17\n","[epoch : 10 / 200] Train Loss : 3605.7139521059785\n","[epoch : 20 / 200] Train Loss : 3473.572796365489\n","[epoch : 30 / 200] Train Loss : 3486.779551630435\n","[epoch : 40 / 200] Train Loss : 3434.2071161684785\n","[epoch : 50 / 200] Train Loss : 3361.511188009511\n","[epoch : 60 / 200] Train Loss : 3442.93751061481\n","[epoch : 70 / 200] Train Loss : 3362.3533351732335\n","[epoch : 80 / 200] Train Loss : 3342.777290675951\n","[epoch : 90 / 200] Train Loss : 3288.041111158288\n","[epoch : 100 / 200] Train Loss : 3326.7877250339675\n","[epoch : 110 / 200] Train Loss : 3321.5581797724185\n","[epoch : 120 / 200] Train Loss : 3286.190387228261\n","[epoch : 130 / 200] Train Loss : 3296.738673997962\n","[epoch : 140 / 200] Train Loss : 3307.12889563519\n","[epoch : 150 / 200] Train Loss : 3307.80225670856\n","[epoch : 160 / 200] Train Loss : 3255.207657523777\n","[epoch : 170 / 200] Train Loss : 3253.492527173913\n","[epoch : 180 / 200] Train Loss : 3300.8797660495925\n","[epoch : 190 / 200] Train Loss : 3264.300154976223\n","[epoch : 200 / 200] Train Loss : 3261.0186714504075\n","Train Time: 33.299352169036865\n","Save Result set: 0, item: 18\n","Save Result set: 1, item: 18\n","Save Result set: 2, item: 18\n","Save Result set: 3, item: 18\n","Save Result set: 4, item: 18\n","Save Result set: 5, item: 18\n","Save Result set: 6, item: 18\n","Save Result set: 7, item: 18\n","Save Result set: 8, item: 18\n","Save Result set: 9, item: 18\n","[epoch : 10 / 200] Train Loss : 603.0108403744905\n","[epoch : 20 / 200] Train Loss : 576.8224009638247\n","[epoch : 30 / 200] Train Loss : 574.4583395253057\n","[epoch : 40 / 200] Train Loss : 564.0967659328295\n","[epoch : 50 / 200] Train Loss : 548.5517259680706\n","[epoch : 60 / 200] Train Loss : 542.8130824876869\n","[epoch : 70 / 200] Train Loss : 546.3387318486753\n","[epoch : 80 / 200] Train Loss : 538.0374716053839\n","[epoch : 90 / 200] Train Loss : 534.3352395762568\n","[epoch : 100 / 200] Train Loss : 540.3520361858865\n","[epoch : 110 / 200] Train Loss : 538.5523044752038\n","[epoch : 120 / 200] Train Loss : 529.1817626953125\n","[epoch : 130 / 200] Train Loss : 528.6851249363111\n","[epoch : 140 / 200] Train Loss : 533.8175831670346\n","[epoch : 150 / 200] Train Loss : 523.8708628778872\n","[epoch : 160 / 200] Train Loss : 533.1517970872962\n","[epoch : 170 / 200] Train Loss : 522.3536177925441\n","[epoch : 180 / 200] Train Loss : 524.6588572626529\n","[epoch : 190 / 200] Train Loss : 520.6593269679857\n","[epoch : 200 / 200] Train Loss : 520.767530358356\n","Train Time: 33.360039472579956\n","Save Result set: 0, item: 19\n","Save Result set: 1, item: 19\n","Save Result set: 2, item: 19\n","Save Result set: 3, item: 19\n","Save Result set: 4, item: 19\n","Save Result set: 5, item: 19\n","Save Result set: 6, item: 19\n","Save Result set: 7, item: 19\n","Save Result set: 8, item: 19\n","Save Result set: 9, item: 19\n","[epoch : 10 / 200] Train Loss : 523.8981402853261\n","[epoch : 20 / 200] Train Loss : 492.158399498981\n","[epoch : 30 / 200] Train Loss : 483.3820615022079\n","[epoch : 40 / 200] Train Loss : 480.00309023649794\n","[epoch : 50 / 200] Train Loss : 469.7265850564708\n","[epoch : 60 / 200] Train Loss : 473.20444123641306\n","[epoch : 70 / 200] Train Loss : 466.99564394743544\n","[epoch : 80 / 200] Train Loss : 465.6547174868376\n","[epoch : 90 / 200] Train Loss : 454.7313829504925\n","[epoch : 100 / 200] Train Loss : 460.53891521951425\n","[epoch : 110 / 200] Train Loss : 452.3916214652683\n","[epoch : 120 / 200] Train Loss : 451.203905188519\n","[epoch : 130 / 200] Train Loss : 453.7425736137058\n","[epoch : 140 / 200] Train Loss : 446.81350309952444\n","[epoch : 150 / 200] Train Loss : 446.5794399095618\n","[epoch : 160 / 200] Train Loss : 442.7490937606148\n","[epoch : 170 / 200] Train Loss : 440.62084695567256\n","[epoch : 180 / 200] Train Loss : 443.7491574494735\n","[epoch : 190 / 200] Train Loss : 442.02019069505775\n","[epoch : 200 / 200] Train Loss : 437.2365085767663\n","Train Time: 33.410706758499146\n","Save Result set: 0, item: 20\n","Save Result set: 1, item: 20\n","Save Result set: 2, item: 20\n","Save Result set: 3, item: 20\n","Save Result set: 4, item: 20\n","Save Result set: 5, item: 20\n","Save Result set: 6, item: 20\n","Save Result set: 7, item: 20\n","Save Result set: 8, item: 20\n","Save Result set: 9, item: 20\n","[epoch : 10 / 200] Train Loss : 234.19633351201597\n","[epoch : 20 / 200] Train Loss : 218.0939470374066\n","[epoch : 30 / 200] Train Loss : 212.5897336213485\n","[epoch : 40 / 200] Train Loss : 212.15060424804688\n","[epoch : 50 / 200] Train Loss : 203.38749562139097\n","[epoch : 60 / 200] Train Loss : 199.3054610542629\n","[epoch : 70 / 200] Train Loss : 198.2203647779382\n","[epoch : 80 / 200] Train Loss : 199.63917939559272\n","[epoch : 90 / 200] Train Loss : 194.7806396484375\n","[epoch : 100 / 200] Train Loss : 192.59742073390794\n","[epoch : 110 / 200] Train Loss : 193.3837777842646\n","[epoch : 120 / 200] Train Loss : 191.2068561056386\n","[epoch : 130 / 200] Train Loss : 189.4407368535581\n","[epoch : 140 / 200] Train Loss : 187.7834300165591\n","[epoch : 150 / 200] Train Loss : 186.54422660495925\n","[epoch : 160 / 200] Train Loss : 189.620093304178\n","[epoch : 170 / 200] Train Loss : 188.06001480765966\n","[epoch : 180 / 200] Train Loss : 187.04563903808594\n","[epoch : 190 / 200] Train Loss : 187.4701810090438\n","[epoch : 200 / 200] Train Loss : 186.86606166673744\n","Train Time: 33.94652557373047\n","Save Result set: 0, item: 21\n","Save Result set: 1, item: 21\n","Save Result set: 2, item: 21\n","Save Result set: 3, item: 21\n","Save Result set: 4, item: 21\n","Save Result set: 5, item: 21\n","Save Result set: 6, item: 21\n","Save Result set: 7, item: 21\n","Save Result set: 8, item: 21\n","Save Result set: 9, item: 21\n","[epoch : 10 / 200] Train Loss : 221.6084747314453\n","[epoch : 20 / 200] Train Loss : 204.0399461829144\n","[epoch : 30 / 200] Train Loss : 198.85432699452275\n","[epoch : 40 / 200] Train Loss : 197.6907388438349\n","[epoch : 50 / 200] Train Loss : 194.3780928902004\n","[epoch : 60 / 200] Train Loss : 189.91451760996944\n","[epoch : 70 / 200] Train Loss : 189.23610057001528\n","[epoch : 80 / 200] Train Loss : 190.6431606126868\n","[epoch : 90 / 200] Train Loss : 185.51800669794497\n","[epoch : 100 / 200] Train Loss : 184.86784561820653\n","[epoch : 110 / 200] Train Loss : 183.85323963994566\n","[epoch : 120 / 200] Train Loss : 181.33919292947522\n","[epoch : 130 / 200] Train Loss : 179.46096470045006\n","[epoch : 140 / 200] Train Loss : 179.28533869204313\n","[epoch : 150 / 200] Train Loss : 180.29330444335938\n","[epoch : 160 / 200] Train Loss : 180.52740942913553\n","[epoch : 170 / 200] Train Loss : 177.20197793711787\n","[epoch : 180 / 200] Train Loss : 177.52996826171875\n","[epoch : 190 / 200] Train Loss : 178.26668581755266\n","[epoch : 200 / 200] Train Loss : 174.3274575938349\n","Train Time: 33.35888671875\n","Save Result set: 0, item: 22\n","Save Result set: 1, item: 22\n","Save Result set: 2, item: 22\n","Save Result set: 3, item: 22\n","Save Result set: 4, item: 22\n","Save Result set: 5, item: 22\n","Save Result set: 6, item: 22\n","Save Result set: 7, item: 22\n","Save Result set: 8, item: 22\n","Save Result set: 9, item: 22\n","[epoch : 10 / 200] Train Loss : 1401.753423276155\n","[epoch : 20 / 200] Train Loss : 1342.734916355299\n","[epoch : 30 / 200] Train Loss : 1306.968505859375\n","[epoch : 40 / 200] Train Loss : 1269.5166280995245\n","[epoch : 50 / 200] Train Loss : 1276.1218367866848\n","[epoch : 60 / 200] Train Loss : 1246.03173828125\n","[epoch : 70 / 200] Train Loss : 1241.598537279212\n","[epoch : 80 / 200] Train Loss : 1230.7252728006115\n","[epoch : 90 / 200] Train Loss : 1211.677049719769\n","[epoch : 100 / 200] Train Loss : 1210.6725410793138\n","[epoch : 110 / 200] Train Loss : 1225.4796912151835\n","[epoch : 120 / 200] Train Loss : 1210.601663340693\n","[epoch : 130 / 200] Train Loss : 1191.9300722868545\n","[epoch : 140 / 200] Train Loss : 1181.430396038553\n","[epoch : 150 / 200] Train Loss : 1187.7951049804688\n","[epoch : 160 / 200] Train Loss : 1180.7518602454145\n","[epoch : 170 / 200] Train Loss : 1165.2383077870245\n","[epoch : 180 / 200] Train Loss : 1179.1615600585938\n","[epoch : 190 / 200] Train Loss : 1152.2716170601223\n","[epoch : 200 / 200] Train Loss : 1167.118973441746\n","Train Time: 33.41004014015198\n","Save Result set: 0, item: 23\n","Save Result set: 1, item: 23\n","Save Result set: 2, item: 23\n","Save Result set: 3, item: 23\n","Save Result set: 4, item: 23\n","Save Result set: 5, item: 23\n","Save Result set: 6, item: 23\n","Save Result set: 7, item: 23\n","Save Result set: 8, item: 23\n","Save Result set: 9, item: 23\n","[epoch : 10 / 200] Train Loss : 360.9325136931046\n","[epoch : 20 / 200] Train Loss : 346.5178156313689\n","[epoch : 30 / 200] Train Loss : 341.35366290548575\n","[epoch : 40 / 200] Train Loss : 331.96816485861075\n","[epoch : 50 / 200] Train Loss : 328.0057081139606\n","[epoch : 60 / 200] Train Loss : 326.23799995754075\n","[epoch : 70 / 200] Train Loss : 325.9094556725544\n","[epoch : 80 / 200] Train Loss : 319.11271999193275\n","[epoch : 90 / 200] Train Loss : 318.1961431088655\n","[epoch : 100 / 200] Train Loss : 318.4091398819633\n","[epoch : 110 / 200] Train Loss : 314.1460014011549\n","[epoch : 120 / 200] Train Loss : 320.6224046790081\n","[epoch : 130 / 200] Train Loss : 311.8058803392493\n","[epoch : 140 / 200] Train Loss : 311.0339819866678\n","[epoch : 150 / 200] Train Loss : 313.16888427734375\n","[epoch : 160 / 200] Train Loss : 306.7748718261719\n","[epoch : 170 / 200] Train Loss : 307.58679066533625\n","[epoch : 180 / 200] Train Loss : 306.7731416121773\n","[epoch : 190 / 200] Train Loss : 303.61512690005094\n","[epoch : 200 / 200] Train Loss : 308.8660424273947\n","Train Time: 33.46796536445618\n","Save Result set: 0, item: 24\n","Save Result set: 1, item: 24\n","Save Result set: 2, item: 24\n","Save Result set: 3, item: 24\n","Save Result set: 4, item: 24\n","Save Result set: 5, item: 24\n","Save Result set: 6, item: 24\n","Save Result set: 7, item: 24\n","Save Result set: 8, item: 24\n","Save Result set: 9, item: 24\n","[epoch : 10 / 200] Train Loss : 947.799120032269\n","[epoch : 20 / 200] Train Loss : 905.3404832922894\n","[epoch : 30 / 200] Train Loss : 887.6085337763247\n","[epoch : 40 / 200] Train Loss : 857.9180855129076\n","[epoch : 50 / 200] Train Loss : 883.7363652768342\n","[epoch : 60 / 200] Train Loss : 874.9407401706861\n","[epoch : 70 / 200] Train Loss : 855.6317430579144\n","[epoch : 80 / 200] Train Loss : 848.2074452275815\n","[epoch : 90 / 200] Train Loss : 853.2711553158967\n","[epoch : 100 / 200] Train Loss : 838.2701601774796\n","[epoch : 110 / 200] Train Loss : 830.5839790675951\n","[epoch : 120 / 200] Train Loss : 838.659352178159\n","[epoch : 130 / 200] Train Loss : 824.6357023819634\n","[epoch : 140 / 200] Train Loss : 839.8039338485054\n","[epoch : 150 / 200] Train Loss : 833.4730675738791\n","[epoch : 160 / 200] Train Loss : 822.1890338400136\n","[epoch : 170 / 200] Train Loss : 828.627513056216\n","[epoch : 180 / 200] Train Loss : 834.9166365913723\n","[epoch : 190 / 200] Train Loss : 804.0570254118546\n","[epoch : 200 / 200] Train Loss : 820.7805626910666\n","Train Time: 34.60501146316528\n","Save Result set: 0, item: 25\n","Save Result set: 1, item: 25\n","Save Result set: 2, item: 25\n","Save Result set: 3, item: 25\n","Save Result set: 4, item: 25\n","Save Result set: 5, item: 25\n","Save Result set: 6, item: 25\n","Save Result set: 7, item: 25\n","Save Result set: 8, item: 25\n","Save Result set: 9, item: 25\n","[epoch : 10 / 200] Train Loss : 958.4947589376699\n","[epoch : 20 / 200] Train Loss : 916.0085051163384\n","[epoch : 30 / 200] Train Loss : 903.7392365828804\n","[epoch : 40 / 200] Train Loss : 896.3755652386209\n","[epoch : 50 / 200] Train Loss : 881.9959610648777\n","[epoch : 60 / 200] Train Loss : 897.9990048615829\n","[epoch : 70 / 200] Train Loss : 884.8304549507473\n","[epoch : 80 / 200] Train Loss : 879.3455571713655\n","[epoch : 90 / 200] Train Loss : 868.8135137143342\n","[epoch : 100 / 200] Train Loss : 865.5145502505095\n","[epoch : 110 / 200] Train Loss : 875.5012339716372\n","[epoch : 120 / 200] Train Loss : 857.2294258449389\n","[epoch : 130 / 200] Train Loss : 853.1810621178669\n","[epoch : 140 / 200] Train Loss : 851.1247823963995\n","[epoch : 150 / 200] Train Loss : 855.7564617654551\n","[epoch : 160 / 200] Train Loss : 853.4369692595109\n","[epoch : 170 / 200] Train Loss : 834.1924385402514\n","[epoch : 180 / 200] Train Loss : 844.789258873981\n","[epoch : 190 / 200] Train Loss : 838.432861328125\n","[epoch : 200 / 200] Train Loss : 830.9081978175951\n","Train Time: 33.842485427856445\n","Save Result set: 0, item: 26\n","Save Result set: 1, item: 26\n","Save Result set: 2, item: 26\n","Save Result set: 3, item: 26\n","Save Result set: 4, item: 26\n","Save Result set: 5, item: 26\n","Save Result set: 6, item: 26\n","Save Result set: 7, item: 26\n","Save Result set: 8, item: 26\n","Save Result set: 9, item: 26\n","[epoch : 10 / 200] Train Loss : 1863.1501305621603\n","[epoch : 20 / 200] Train Loss : 1811.745706309443\n","[epoch : 30 / 200] Train Loss : 1780.769828464674\n","[epoch : 40 / 200] Train Loss : 1771.4793276579483\n","[epoch : 50 / 200] Train Loss : 1711.1322870669158\n","[epoch : 60 / 200] Train Loss : 1724.1532407014267\n","[epoch : 70 / 200] Train Loss : 1722.0917756453805\n","[epoch : 80 / 200] Train Loss : 1681.655660878057\n","[epoch : 90 / 200] Train Loss : 1684.707036557405\n","[epoch : 100 / 200] Train Loss : 1695.554735266644\n","[epoch : 110 / 200] Train Loss : 1694.7150188943615\n","[epoch : 120 / 200] Train Loss : 1677.9794072690217\n","[epoch : 130 / 200] Train Loss : 1666.4675505264945\n","[epoch : 140 / 200] Train Loss : 1655.0319930366848\n","[epoch : 150 / 200] Train Loss : 1636.651319420856\n","[epoch : 160 / 200] Train Loss : 1656.4840990149457\n","[epoch : 170 / 200] Train Loss : 1624.8728398862092\n","[epoch : 180 / 200] Train Loss : 1637.6931523862092\n","[epoch : 190 / 200] Train Loss : 1600.028856360394\n","[epoch : 200 / 200] Train Loss : 1624.7881071671195\n","Train Time: 33.21160864830017\n","Save Result set: 0, item: 27\n","Save Result set: 1, item: 27\n","Save Result set: 2, item: 27\n","Save Result set: 3, item: 27\n","Save Result set: 4, item: 27\n","Save Result set: 5, item: 27\n","Save Result set: 6, item: 27\n","Save Result set: 7, item: 27\n","Save Result set: 8, item: 27\n","Save Result set: 9, item: 27\n","[epoch : 10 / 200] Train Loss : 189.03739730171534\n","[epoch : 20 / 200] Train Loss : 175.30648272970447\n","[epoch : 30 / 200] Train Loss : 169.7191792363706\n","[epoch : 40 / 200] Train Loss : 168.45687070100203\n","[epoch : 50 / 200] Train Loss : 167.22459079908288\n","[epoch : 60 / 200] Train Loss : 166.59012968643853\n","[epoch : 70 / 200] Train Loss : 161.7182106349779\n","[epoch : 80 / 200] Train Loss : 161.83794436247453\n","[epoch : 90 / 200] Train Loss : 160.7871153458305\n","[epoch : 100 / 200] Train Loss : 158.89310289465863\n","[epoch : 110 / 200] Train Loss : 157.2825768512228\n","[epoch : 120 / 200] Train Loss : 157.7405123503312\n","[epoch : 130 / 200] Train Loss : 155.48818870212722\n","[epoch : 140 / 200] Train Loss : 155.82965220575747\n","[epoch : 150 / 200] Train Loss : 154.68305504840353\n","[epoch : 160 / 200] Train Loss : 154.95781873620075\n","[epoch : 170 / 200] Train Loss : 153.48564546004584\n","[epoch : 180 / 200] Train Loss : 154.12762384829313\n","[epoch : 190 / 200] Train Loss : 151.77042024031928\n","[epoch : 200 / 200] Train Loss : 154.4204924210258\n","Train Time: 33.48098134994507\n","Save Result set: 0, item: 28\n","Save Result set: 1, item: 28\n","Save Result set: 2, item: 28\n","Save Result set: 3, item: 28\n","Save Result set: 4, item: 28\n","Save Result set: 5, item: 28\n","Save Result set: 6, item: 28\n","Save Result set: 7, item: 28\n","Save Result set: 8, item: 28\n","Save Result set: 9, item: 28\n","[epoch : 10 / 200] Train Loss : 338.16554857336956\n","[epoch : 20 / 200] Train Loss : 321.98432590650475\n","[epoch : 30 / 200] Train Loss : 309.8201837954314\n","[epoch : 40 / 200] Train Loss : 310.87881735096806\n","[epoch : 50 / 200] Train Loss : 304.3525735606318\n","[epoch : 60 / 200] Train Loss : 302.21881103515625\n","[epoch : 70 / 200] Train Loss : 304.7801672894022\n","[epoch : 80 / 200] Train Loss : 291.8716941501783\n","[epoch : 90 / 200] Train Loss : 295.2507602857507\n","[epoch : 100 / 200] Train Loss : 295.1423021399456\n","[epoch : 110 / 200] Train Loss : 290.14123402471125\n","[epoch : 120 / 200] Train Loss : 291.3408150050951\n","[epoch : 130 / 200] Train Loss : 291.9091849949049\n","[epoch : 140 / 200] Train Loss : 289.7311825959579\n","[epoch : 150 / 200] Train Loss : 286.0563500445822\n","[epoch : 160 / 200] Train Loss : 285.2810124936311\n","[epoch : 170 / 200] Train Loss : 281.1041982899541\n","[epoch : 180 / 200] Train Loss : 277.7749779742697\n","[epoch : 190 / 200] Train Loss : 279.1381769594939\n","[epoch : 200 / 200] Train Loss : 276.9785892652429\n","Train Time: 33.954259634017944\n","Save Result set: 0, item: 29\n","Save Result set: 1, item: 29\n","Save Result set: 2, item: 29\n","Save Result set: 3, item: 29\n","Save Result set: 4, item: 29\n","Save Result set: 5, item: 29\n","Save Result set: 6, item: 29\n","Save Result set: 7, item: 29\n","Save Result set: 8, item: 29\n","Save Result set: 9, item: 29\n","[epoch : 10 / 200] Train Loss : 239.0772187606148\n","[epoch : 20 / 200] Train Loss : 218.68575519064197\n","[epoch : 30 / 200] Train Loss : 213.56615282141644\n","[epoch : 40 / 200] Train Loss : 205.46261198624322\n","[epoch : 50 / 200] Train Loss : 207.22787276558253\n","[epoch : 60 / 200] Train Loss : 200.8559112548828\n","[epoch : 70 / 200] Train Loss : 201.14713121497113\n","[epoch : 80 / 200] Train Loss : 196.2961206850798\n","[epoch : 90 / 200] Train Loss : 194.83182426120925\n","[epoch : 100 / 200] Train Loss : 196.5026457413383\n","[epoch : 110 / 200] Train Loss : 192.75326339058253\n","[epoch : 120 / 200] Train Loss : 192.6786399509596\n","[epoch : 130 / 200] Train Loss : 193.78646585215694\n","[epoch : 140 / 200] Train Loss : 198.67965034816575\n","[epoch : 150 / 200] Train Loss : 188.4342624830163\n","[epoch : 160 / 200] Train Loss : 188.54268281356147\n","[epoch : 170 / 200] Train Loss : 190.38161236306897\n","[epoch : 180 / 200] Train Loss : 188.95950715438178\n","[epoch : 190 / 200] Train Loss : 185.6765766973081\n","[epoch : 200 / 200] Train Loss : 182.69201991869056\n","Train Time: 33.361711502075195\n","Save Result set: 0, item: 30\n","Save Result set: 1, item: 30\n","Save Result set: 2, item: 30\n","Save Result set: 3, item: 30\n","Save Result set: 4, item: 30\n","Save Result set: 5, item: 30\n","Save Result set: 6, item: 30\n","Save Result set: 7, item: 30\n","Save Result set: 8, item: 30\n","Save Result set: 9, item: 30\n","[epoch : 10 / 200] Train Loss : 1200.7214461616848\n","[epoch : 20 / 200] Train Loss : 1171.8648522418478\n","[epoch : 30 / 200] Train Loss : 1154.1692531419837\n","[epoch : 40 / 200] Train Loss : 1126.4812462848165\n","[epoch : 50 / 200] Train Loss : 1111.9740388289742\n","[epoch : 60 / 200] Train Loss : 1107.7206182065217\n","[epoch : 70 / 200] Train Loss : 1084.9755753226902\n","[epoch : 80 / 200] Train Loss : 1080.8317021908967\n","[epoch : 90 / 200] Train Loss : 1069.742134425951\n","[epoch : 100 / 200] Train Loss : 1055.6971594769022\n","[epoch : 110 / 200] Train Loss : 1060.307296089504\n","[epoch : 120 / 200] Train Loss : 1052.6410814368207\n","[epoch : 130 / 200] Train Loss : 1079.8074526579483\n","[epoch : 140 / 200] Train Loss : 1048.631830630095\n","[epoch : 150 / 200] Train Loss : 1021.2264059315557\n","[epoch : 160 / 200] Train Loss : 1036.4673966117527\n","[epoch : 170 / 200] Train Loss : 1028.9107957922895\n","[epoch : 180 / 200] Train Loss : 1018.1948295261549\n","[epoch : 190 / 200] Train Loss : 1027.2149233610733\n","[epoch : 200 / 200] Train Loss : 1009.0631793478261\n","Train Time: 33.26167273521423\n","Save Result set: 0, item: 31\n","Save Result set: 1, item: 31\n","Save Result set: 2, item: 31\n","Save Result set: 3, item: 31\n","Save Result set: 4, item: 31\n","Save Result set: 5, item: 31\n","Save Result set: 6, item: 31\n","Save Result set: 7, item: 31\n","Save Result set: 8, item: 31\n","Save Result set: 9, item: 31\n","[epoch : 10 / 200] Train Loss : 1110.770579462466\n","[epoch : 20 / 200] Train Loss : 1073.259545367697\n","[epoch : 30 / 200] Train Loss : 1056.3547734799592\n","[epoch : 40 / 200] Train Loss : 1027.5658383576767\n","[epoch : 50 / 200] Train Loss : 1021.2751677139946\n","[epoch : 60 / 200] Train Loss : 1015.7657258406929\n","[epoch : 70 / 200] Train Loss : 1039.189500891644\n","[epoch : 80 / 200] Train Loss : 1021.0261761209239\n","[epoch : 90 / 200] Train Loss : 1007.5864390497622\n","[epoch : 100 / 200] Train Loss : 1011.4048833432405\n","[epoch : 110 / 200] Train Loss : 1008.3661737856658\n","[epoch : 120 / 200] Train Loss : 1000.133008873981\n","[epoch : 130 / 200] Train Loss : 993.2138724949049\n","[epoch : 140 / 200] Train Loss : 980.7257027004076\n","[epoch : 150 / 200] Train Loss : 989.5490404211956\n","[epoch : 160 / 200] Train Loss : 987.272312330163\n","[epoch : 170 / 200] Train Loss : 975.265869140625\n","[epoch : 180 / 200] Train Loss : 992.936791461447\n","[epoch : 190 / 200] Train Loss : 976.4410055409307\n","[epoch : 200 / 200] Train Loss : 984.2003253439199\n","Train Time: 33.47228789329529\n","Save Result set: 0, item: 32\n","Save Result set: 1, item: 32\n","Save Result set: 2, item: 32\n","Save Result set: 3, item: 32\n","Save Result set: 4, item: 32\n","Save Result set: 5, item: 32\n","Save Result set: 6, item: 32\n","Save Result set: 7, item: 32\n","Save Result set: 8, item: 32\n","Save Result set: 9, item: 32\n","[epoch : 10 / 200] Train Loss : 1740.3032438858695\n","[epoch : 20 / 200] Train Loss : 1613.7166641898777\n","[epoch : 30 / 200] Train Loss : 1602.9810366423233\n","[epoch : 40 / 200] Train Loss : 1578.1803296959918\n","[epoch : 50 / 200] Train Loss : 1544.4607198963995\n","[epoch : 60 / 200] Train Loss : 1539.9724545686142\n","[epoch : 70 / 200] Train Loss : 1547.495260487432\n","[epoch : 80 / 200] Train Loss : 1546.993896484375\n","[epoch : 90 / 200] Train Loss : 1529.4022057574728\n","[epoch : 100 / 200] Train Loss : 1517.8027502972147\n","[epoch : 110 / 200] Train Loss : 1516.0002282184103\n","[epoch : 120 / 200] Train Loss : 1504.4955417798913\n","[epoch : 130 / 200] Train Loss : 1488.169874108356\n","[epoch : 140 / 200] Train Loss : 1486.7302033797555\n","[epoch : 150 / 200] Train Loss : 1475.373927904212\n","[epoch : 160 / 200] Train Loss : 1502.7081192680027\n","[epoch : 170 / 200] Train Loss : 1458.2979683254075\n","[epoch : 180 / 200] Train Loss : 1454.2143820057745\n","[epoch : 190 / 200] Train Loss : 1449.4590162194293\n","[epoch : 200 / 200] Train Loss : 1471.2019202190897\n","Train Time: 33.53402853012085\n","Save Result set: 0, item: 33\n","Save Result set: 1, item: 33\n","Save Result set: 2, item: 33\n","Save Result set: 3, item: 33\n","Save Result set: 4, item: 33\n","Save Result set: 5, item: 33\n","Save Result set: 6, item: 33\n","Save Result set: 7, item: 33\n","Save Result set: 8, item: 33\n","Save Result set: 9, item: 33\n","[epoch : 10 / 200] Train Loss : 1222.0151207965353\n","[epoch : 20 / 200] Train Loss : 1174.30199664572\n","[epoch : 30 / 200] Train Loss : 1176.0885567043138\n","[epoch : 40 / 200] Train Loss : 1156.657685653023\n","[epoch : 50 / 200] Train Loss : 1127.927150560462\n","[epoch : 60 / 200] Train Loss : 1121.9071522588315\n","[epoch : 70 / 200] Train Loss : 1125.771120817765\n","[epoch : 80 / 200] Train Loss : 1123.2366784137228\n","[epoch : 90 / 200] Train Loss : 1107.5049863068955\n","[epoch : 100 / 200] Train Loss : 1110.9277556046195\n","[epoch : 110 / 200] Train Loss : 1105.8454191788383\n","[epoch : 120 / 200] Train Loss : 1114.3070227581522\n","[epoch : 130 / 200] Train Loss : 1084.2817595108695\n","[epoch : 140 / 200] Train Loss : 1109.4255769149117\n","[epoch : 150 / 200] Train Loss : 1106.014016856318\n","[epoch : 160 / 200] Train Loss : 1083.2504617442255\n","[epoch : 170 / 200] Train Loss : 1080.9824298361073\n","[epoch : 180 / 200] Train Loss : 1086.8058445142663\n","[epoch : 190 / 200] Train Loss : 1086.526462720788\n","[epoch : 200 / 200] Train Loss : 1071.949969747792\n","Train Time: 33.36726450920105\n","Save Result set: 0, item: 34\n","Save Result set: 1, item: 34\n","Save Result set: 2, item: 34\n","Save Result set: 3, item: 34\n","Save Result set: 4, item: 34\n","Save Result set: 5, item: 34\n","Save Result set: 6, item: 34\n","Save Result set: 7, item: 34\n","Save Result set: 8, item: 34\n","Save Result set: 9, item: 34\n","[epoch : 10 / 200] Train Loss : 751.7586669921875\n","[epoch : 20 / 200] Train Loss : 690.2295903744905\n","[epoch : 30 / 200] Train Loss : 687.0666610054348\n","[epoch : 40 / 200] Train Loss : 677.9447791058084\n","[epoch : 50 / 200] Train Loss : 666.3109555451766\n","[epoch : 60 / 200] Train Loss : 669.301489788553\n","[epoch : 70 / 200] Train Loss : 662.7985654084579\n","[epoch : 80 / 200] Train Loss : 654.5113339631454\n","[epoch : 90 / 200] Train Loss : 644.7055491571841\n","[epoch : 100 / 200] Train Loss : 651.6239013671875\n","[epoch : 110 / 200] Train Loss : 645.7463431980299\n","[epoch : 120 / 200] Train Loss : 655.4793462338655\n","[epoch : 130 / 200] Train Loss : 653.4033096976902\n","[epoch : 140 / 200] Train Loss : 641.9840220575747\n","[epoch : 150 / 200] Train Loss : 640.7660442849864\n","[epoch : 160 / 200] Train Loss : 638.1123365319294\n","[epoch : 170 / 200] Train Loss : 639.4580582328465\n","[epoch : 180 / 200] Train Loss : 631.0527025305706\n","[epoch : 190 / 200] Train Loss : 625.0012233568275\n","[epoch : 200 / 200] Train Loss : 626.1311618970788\n","Train Time: 33.35497736930847\n","Save Result set: 0, item: 35\n","Save Result set: 1, item: 35\n","Save Result set: 2, item: 35\n","Save Result set: 3, item: 35\n","Save Result set: 4, item: 35\n","Save Result set: 5, item: 35\n","Save Result set: 6, item: 35\n","Save Result set: 7, item: 35\n","Save Result set: 8, item: 35\n","Save Result set: 9, item: 35\n","[epoch : 10 / 200] Train Loss : 582.6489178201426\n","[epoch : 20 / 200] Train Loss : 548.9128922172215\n","[epoch : 30 / 200] Train Loss : 528.9690604831861\n","[epoch : 40 / 200] Train Loss : 525.2665352199389\n","[epoch : 50 / 200] Train Loss : 513.5328846807065\n","[epoch : 60 / 200] Train Loss : 521.9481121560801\n","[epoch : 70 / 200] Train Loss : 504.3366181746773\n","[epoch : 80 / 200] Train Loss : 514.7875989831012\n","[epoch : 90 / 200] Train Loss : 498.6488912831182\n","[epoch : 100 / 200] Train Loss : 499.59822546917457\n","[epoch : 110 / 200] Train Loss : 488.872766909392\n","[epoch : 120 / 200] Train Loss : 491.4961574388587\n","[epoch : 130 / 200] Train Loss : 495.3547801142154\n","[epoch : 140 / 200] Train Loss : 486.6464910092561\n","[epoch : 150 / 200] Train Loss : 484.7480813731318\n","[epoch : 160 / 200] Train Loss : 489.3271205736243\n","[epoch : 170 / 200] Train Loss : 483.0515415357507\n","[epoch : 180 / 200] Train Loss : 479.4541055430537\n","[epoch : 190 / 200] Train Loss : 475.77627828846806\n","[epoch : 200 / 200] Train Loss : 480.40069978133494\n","Train Time: 33.393837451934814\n","Save Result set: 0, item: 36\n","Save Result set: 1, item: 36\n","Save Result set: 2, item: 36\n","Save Result set: 3, item: 36\n","Save Result set: 4, item: 36\n","Save Result set: 5, item: 36\n","Save Result set: 6, item: 36\n","Save Result set: 7, item: 36\n","Save Result set: 8, item: 36\n","Save Result set: 9, item: 36\n"]}]},{"cell_type":"markdown","source":["Test Output"],"metadata":{"id":"vITz1CJJhLon"}},{"cell_type":"code","source":["for k in tqdm(range(10)):\n","\n","  globals()[f'set_df_{k}'] = pd.DataFrame()\n","  answer_df_list = glob(f'./set_{k}/*.csv') # 예측한 결과 불러오기\n","  pum_list = glob(f'./aT_test_raw/sep_{k}/*.csv') # 기존 test input 불러오기\n","  pummok = [a for a in pum_list if 'pummok' in a.split('/')[-1]]\n","\n","  for i in answer_df_list:\n","    df = pd.read_csv(i)\n","    number = i.split('_')[-1].split('.')[0]\n","\n","    base_number = 0\n","    for p in pummok:\n","      if number == p.split('_')[-1].split('.')[0]:\n","        pum_df = pd.read_csv(p)\n","\n","        if len(pum_df) != 0:\n","           base_number = pum_df.iloc[len(pum_df)-1]['해당일자_전체평균가격(원)']  # 기존 각 sep 마다 test input의 마지막 target 값 가져오기 (변동률 계산을 위해)\n","        else:\n","          base_number = np.nan\n","\n","    globals()[f'set_df_{k}'][f'품목{number}']  = [base_number] + list(df[df.columns[-1]].values) # 각 품목당 순서를 t, t+1 ... t+28 로 변경\n","\n","  globals()[f'set_df_{k}'] = globals()[f'set_df_{k}'][[f'품목{col}' for col in range(37)]] # 열 순서를 품목0 ~ 품목36 으로 변경"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pfSjhCREm0Ws","executionInfo":{"status":"ok","timestamp":1662482542857,"user_tz":-540,"elapsed":42613,"user":{"displayName":"김성찬","userId":"03517507033102912155"}},"outputId":"37c84c09-67a2-4fdf-e323-d5125fea96fb"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 10/10 [00:42<00:00,  4.22s/it]\n"]}]},{"cell_type":"code","source":["set_df_0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"AGC82Kx8m1so","executionInfo":{"status":"ok","timestamp":1662482545670,"user_tz":-540,"elapsed":387,"user":{"displayName":"김성찬","userId":"03517507033102912155"}},"outputId":"1768a85a-4530-4820-a3cb-c4c24187bfae"},"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["          품목0          품목1          품목2          품목3          품목4  \\\n","0   3871.1250  1362.117613  2909.783785  3400.075583  3947.809169   \n","1   2533.7441  2062.236800  2267.717300  1972.134400  1802.643300   \n","2   2542.6628  2138.377400  2117.543200  1771.542800  2023.675800   \n","3   2519.9717  2002.014400  2195.209000  1974.004400  2016.050200   \n","4   2500.0532  2112.552700  2271.998800  1954.208100  1959.919800   \n","5   2566.9224  2182.557600  2182.943000  1925.009500  1849.007200   \n","6   2679.7240  2099.698500  2127.137200  2057.461200  2018.698000   \n","7   2483.4197  2175.900000  2060.634800  1894.889400  1947.581200   \n","8   2704.0930  2079.968500  2160.838100  1938.232900  1900.109400   \n","9   2649.7031  2218.693400  2025.289300  1964.707800  1833.210100   \n","10  2555.4036  2234.234900  2251.957500  2011.093600  2106.621300   \n","11  2391.6406  2110.986000  2153.611000  1871.428800  1977.810400   \n","12  2432.7588  2141.061800  2307.670400  1871.195800  1960.904000   \n","13  2307.2695  2209.284700  2094.340600  2042.292000  1909.829500   \n","14  2512.4536  2184.163600  2289.780300  1964.576700  2117.789000   \n","15  2380.7197  2213.407700  2263.250500  1924.683700  1943.427600   \n","16  2472.6868  2150.970000  2196.457000  1863.737800  1957.257400   \n","17  2643.5916  2109.740200  2213.871600  1923.724600  1944.765300   \n","18  2429.3790  1958.375600  2217.941000  2040.392700  1954.205200   \n","19  2495.6047  2192.069800  2254.957500  1994.835300  1974.915200   \n","20  2479.6357  2081.817000  2192.535400  1971.280600  1947.920000   \n","21  2640.9678  2134.210200  2181.424300  2110.755100  1912.853600   \n","22  2503.6467  1953.460000  2260.532500  2025.342400  1934.826800   \n","23  2507.5457  2106.150600  2228.801000  1947.157600  1909.163200   \n","24  2414.2263  2088.529800  2334.534400  1986.702500  1999.411900   \n","25  2631.5017  2174.263200  2231.679000  2100.790500  1989.730600   \n","26  2667.8171  2068.444800  2082.848400  2069.827000  2070.345500   \n","27  2509.6267  2223.662000  2214.927700  1879.381700  2034.301800   \n","28  2504.5080  2048.426300  2109.233400  1821.881800  1896.296100   \n","\n","            품목5        품목6          품목7          품목8          품목9  ...  \\\n","0   9253.947514  2717.2800  3361.030923  4911.899864  1173.018633  ...   \n","1   3755.937000  4498.7700  1931.120500  4251.740700  1396.946300  ...   \n","2   3726.412000  5145.9062  1962.652600  3732.838600  1434.518700  ...   \n","3   3596.003400  5042.3490  1877.810200  4187.588000  1497.642800  ...   \n","4   3685.122800  5428.8970  1995.662500  3915.843300  1486.948200  ...   \n","5   3693.539800  4877.8354  1843.055800  4126.392000  1494.093900  ...   \n","6   3947.160400  4724.3840  2025.323400  3858.429200  1451.620100  ...   \n","7   3908.217800  4863.4917  1994.662000  4122.034000  1401.648400  ...   \n","8   3699.395300  4840.4300  2016.958400  3846.404300  1469.242600  ...   \n","9   3767.894800  5226.5980  1927.292100  4043.276900  1384.466200  ...   \n","10  3911.747800  4681.8060  2178.151100  3958.223400  1530.339800  ...   \n","11  3897.885500  4614.8125  2018.013300  4091.570300  1400.770500  ...   \n","12  3748.895300  5084.1300  2065.085700  4101.983000  1432.151900  ...   \n","13  3837.052000  4928.9546  1981.891200  4018.949500  1497.190000  ...   \n","14  3819.671400  4917.8460  1953.171800  4121.334000  1516.695700  ...   \n","15  3780.757300  4447.8190  1940.095800  3996.610800  1489.984700  ...   \n","16  3796.887500  4966.7200  1990.416400  3998.694000  1487.574100  ...   \n","17  3561.996800  4661.8784  2096.163600  3747.636500  1468.624000  ...   \n","18  3777.925500  4486.6080  1937.652100  4024.625000  1489.283000  ...   \n","19  3837.778800  4585.0557  2060.752000  4310.134300  1516.349900  ...   \n","20  4193.608000  4446.5190  2028.518700  3901.384300  1541.405600  ...   \n","21  3749.915000  4089.6287  1939.439600  4060.610800  1338.270100  ...   \n","22  3822.831000  4340.5530  1922.243400  3970.466300  1451.545200  ...   \n","23  3850.791300  4099.4350  2100.353000  4090.979500  1519.749300  ...   \n","24  3791.733400  3943.8850  1881.341600  4027.901000  1421.372400  ...   \n","25  3887.336200  4239.7410  2016.432700  3820.346700  1447.595500  ...   \n","26  3840.469500  4038.7314  1870.766600  3762.023700  1423.111500  ...   \n","27  3990.701700  3722.9275  1796.140000  3963.283200  1413.631600  ...   \n","28  3761.522700  3550.6020  1749.937100  4247.510700  1456.532000  ...   \n","\n","           품목27         품목28         품목29         품목30         품목31  \\\n","0   8640.811309   602.005658  1105.412623  1566.274239  3633.464557   \n","1   6249.097700  1107.852700  1113.296500   859.708600  5976.319300   \n","2   6072.542500  1110.131500  1038.418500   848.371030  6355.076000   \n","3   6035.966000  1107.703200  1064.338100   907.723270  5810.241700   \n","4   5891.401000  1065.247400  1042.478300   920.607540  6165.758000   \n","5   5775.318000  1065.535400  1012.271900   853.161500  6058.119000   \n","6   6013.942000  1105.089400  1043.656000   925.716600  5900.512700   \n","7   5925.289000  1091.123300  1042.539700   919.828250  6455.265000   \n","8   5867.860400  1092.424000  1058.137800   844.960800  6483.064500   \n","9   5927.474000  1138.479900  1062.101400   884.120200  6437.127000   \n","10  6610.960000  1163.561200   967.015200   858.660500  6474.855500   \n","11  6305.104000  1131.817500   988.922500   865.094670  6404.954000   \n","12  6432.797000  1158.585300  1106.671000   861.926640  6068.854500   \n","13  6133.165500  1174.436600  1046.371500   852.156860  6036.931000   \n","14  6129.551300  1188.180900   974.227100   822.248540  5952.837000   \n","15  6062.897000  1071.564600  1048.462900   887.632140  6178.382000   \n","16  6318.098600  1155.526500  1033.767700   859.979900  6597.917000   \n","17  6420.741700  1143.257700  1024.603400   901.389160  5931.353000   \n","18  6633.215300  1022.997400  1038.949800   862.217800  6649.317400   \n","19  5894.157000  1150.599600   996.019600   910.999500  6393.194300   \n","20  5885.730500  1055.851200  1064.131800   879.676800  6403.764000   \n","21  6376.803000  1110.459400  1058.477200   853.317570  6546.675300   \n","22  6046.294400  1066.853100  1028.372600   852.377870  6372.842300   \n","23  5927.823000  1108.419400  1041.684800   896.412100  6218.843300   \n","24  6095.906000  1058.309400  1042.541900   867.276730  6158.952000   \n","25  5825.999500  1071.444500  1053.347800   866.366500  5942.574700   \n","26  6272.114300  1111.838700  1133.561500   854.553100  6036.143600   \n","27  5681.063500  1160.893400   980.073550   876.500200  6292.896500   \n","28  6353.352000  1109.217900  1023.777950   879.716700  6470.217300   \n","\n","           품목32         품목33         품목34         품목35         품목36  \n","0   5454.710444  5619.188362  5230.620027  2905.100888  2087.675036  \n","1   2835.676000  3069.145300  3845.493400  2003.704000  2405.182000  \n","2   3044.851600  2789.640000  4049.588100  2012.274300  2336.066000  \n","3   2898.436000  2797.160000  4075.843300  2036.632400  2271.508800  \n","4   2903.377400  2986.417000  3839.330600  1976.353500  2344.689200  \n","5   2965.827600  2885.313200  4086.439700  1899.591100  2264.151900  \n","6   2965.444300  2993.767800  3916.820000  2045.004000  2406.767300  \n","7   2859.611800  2799.460200  4047.219200  2059.743400  2353.426300  \n","8   3133.451200  2849.129400  4029.520300  2112.472700  2427.927000  \n","9   3011.683800  2899.210200  4085.447500  2228.401000  2269.580600  \n","10  2890.810500  3012.551000  4224.013000  2038.122600  2420.568600  \n","11  2978.431600  2805.287000  4038.494000  2140.035600  2433.693600  \n","12  2832.755900  3057.544700  3857.684000  2061.735000  2288.010500  \n","13  2983.566200  2800.511700  3956.231700  2061.298600  2418.277000  \n","14  2793.019500  2990.980000  3929.864300  1951.214700  2372.516600  \n","15  2903.879200  2917.556400  4037.454800  2000.983200  2380.047000  \n","16  2832.113500  2825.306400  3905.120400  1987.797900  2354.281200  \n","17  2776.784400  2815.636200  3878.844500  2161.270300  2262.784200  \n","18  2749.428500  2866.756600  4287.067000  2179.608200  2389.936800  \n","19  2922.202000  2933.635700  3779.452600  2002.221100  2318.298800  \n","20  2985.828100  3026.011000  3899.249000  2075.383800  2281.430700  \n","21  2901.989300  3010.852000  3581.418500  2091.183600  2343.295700  \n","22  2965.545400  3170.115200  3656.383300  2051.069000  2217.139400  \n","23  2891.192100  3015.943000  3882.592800  2069.763400  2427.455800  \n","24  3017.944000  3144.766400  3983.404500  2163.130000  2111.729500  \n","25  2865.497000  3003.416300  3772.822300  1976.094500  2398.749300  \n","26  2927.757800  2996.377400  3870.803700  2012.912400  2442.134500  \n","27  2778.773400  3324.061000  3887.803200  1928.287400  2270.807100  \n","28  3073.331300  2917.789600  3984.705000  2262.620800  2133.361800  \n","\n","[29 rows x 37 columns]"],"text/html":["\n","  <div id=\"df-c6512d95-6540-47c6-8c04-b575af60d1c5\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>품목0</th>\n","      <th>품목1</th>\n","      <th>품목2</th>\n","      <th>품목3</th>\n","      <th>품목4</th>\n","      <th>품목5</th>\n","      <th>품목6</th>\n","      <th>품목7</th>\n","      <th>품목8</th>\n","      <th>품목9</th>\n","      <th>...</th>\n","      <th>품목27</th>\n","      <th>품목28</th>\n","      <th>품목29</th>\n","      <th>품목30</th>\n","      <th>품목31</th>\n","      <th>품목32</th>\n","      <th>품목33</th>\n","      <th>품목34</th>\n","      <th>품목35</th>\n","      <th>품목36</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>3871.1250</td>\n","      <td>1362.117613</td>\n","      <td>2909.783785</td>\n","      <td>3400.075583</td>\n","      <td>3947.809169</td>\n","      <td>9253.947514</td>\n","      <td>2717.2800</td>\n","      <td>3361.030923</td>\n","      <td>4911.899864</td>\n","      <td>1173.018633</td>\n","      <td>...</td>\n","      <td>8640.811309</td>\n","      <td>602.005658</td>\n","      <td>1105.412623</td>\n","      <td>1566.274239</td>\n","      <td>3633.464557</td>\n","      <td>5454.710444</td>\n","      <td>5619.188362</td>\n","      <td>5230.620027</td>\n","      <td>2905.100888</td>\n","      <td>2087.675036</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2533.7441</td>\n","      <td>2062.236800</td>\n","      <td>2267.717300</td>\n","      <td>1972.134400</td>\n","      <td>1802.643300</td>\n","      <td>3755.937000</td>\n","      <td>4498.7700</td>\n","      <td>1931.120500</td>\n","      <td>4251.740700</td>\n","      <td>1396.946300</td>\n","      <td>...</td>\n","      <td>6249.097700</td>\n","      <td>1107.852700</td>\n","      <td>1113.296500</td>\n","      <td>859.708600</td>\n","      <td>5976.319300</td>\n","      <td>2835.676000</td>\n","      <td>3069.145300</td>\n","      <td>3845.493400</td>\n","      <td>2003.704000</td>\n","      <td>2405.182000</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2542.6628</td>\n","      <td>2138.377400</td>\n","      <td>2117.543200</td>\n","      <td>1771.542800</td>\n","      <td>2023.675800</td>\n","      <td>3726.412000</td>\n","      <td>5145.9062</td>\n","      <td>1962.652600</td>\n","      <td>3732.838600</td>\n","      <td>1434.518700</td>\n","      <td>...</td>\n","      <td>6072.542500</td>\n","      <td>1110.131500</td>\n","      <td>1038.418500</td>\n","      <td>848.371030</td>\n","      <td>6355.076000</td>\n","      <td>3044.851600</td>\n","      <td>2789.640000</td>\n","      <td>4049.588100</td>\n","      <td>2012.274300</td>\n","      <td>2336.066000</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2519.9717</td>\n","      <td>2002.014400</td>\n","      <td>2195.209000</td>\n","      <td>1974.004400</td>\n","      <td>2016.050200</td>\n","      <td>3596.003400</td>\n","      <td>5042.3490</td>\n","      <td>1877.810200</td>\n","      <td>4187.588000</td>\n","      <td>1497.642800</td>\n","      <td>...</td>\n","      <td>6035.966000</td>\n","      <td>1107.703200</td>\n","      <td>1064.338100</td>\n","      <td>907.723270</td>\n","      <td>5810.241700</td>\n","      <td>2898.436000</td>\n","      <td>2797.160000</td>\n","      <td>4075.843300</td>\n","      <td>2036.632400</td>\n","      <td>2271.508800</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2500.0532</td>\n","      <td>2112.552700</td>\n","      <td>2271.998800</td>\n","      <td>1954.208100</td>\n","      <td>1959.919800</td>\n","      <td>3685.122800</td>\n","      <td>5428.8970</td>\n","      <td>1995.662500</td>\n","      <td>3915.843300</td>\n","      <td>1486.948200</td>\n","      <td>...</td>\n","      <td>5891.401000</td>\n","      <td>1065.247400</td>\n","      <td>1042.478300</td>\n","      <td>920.607540</td>\n","      <td>6165.758000</td>\n","      <td>2903.377400</td>\n","      <td>2986.417000</td>\n","      <td>3839.330600</td>\n","      <td>1976.353500</td>\n","      <td>2344.689200</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>2566.9224</td>\n","      <td>2182.557600</td>\n","      <td>2182.943000</td>\n","      <td>1925.009500</td>\n","      <td>1849.007200</td>\n","      <td>3693.539800</td>\n","      <td>4877.8354</td>\n","      <td>1843.055800</td>\n","      <td>4126.392000</td>\n","      <td>1494.093900</td>\n","      <td>...</td>\n","      <td>5775.318000</td>\n","      <td>1065.535400</td>\n","      <td>1012.271900</td>\n","      <td>853.161500</td>\n","      <td>6058.119000</td>\n","      <td>2965.827600</td>\n","      <td>2885.313200</td>\n","      <td>4086.439700</td>\n","      <td>1899.591100</td>\n","      <td>2264.151900</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>2679.7240</td>\n","      <td>2099.698500</td>\n","      <td>2127.137200</td>\n","      <td>2057.461200</td>\n","      <td>2018.698000</td>\n","      <td>3947.160400</td>\n","      <td>4724.3840</td>\n","      <td>2025.323400</td>\n","      <td>3858.429200</td>\n","      <td>1451.620100</td>\n","      <td>...</td>\n","      <td>6013.942000</td>\n","      <td>1105.089400</td>\n","      <td>1043.656000</td>\n","      <td>925.716600</td>\n","      <td>5900.512700</td>\n","      <td>2965.444300</td>\n","      <td>2993.767800</td>\n","      <td>3916.820000</td>\n","      <td>2045.004000</td>\n","      <td>2406.767300</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>2483.4197</td>\n","      <td>2175.900000</td>\n","      <td>2060.634800</td>\n","      <td>1894.889400</td>\n","      <td>1947.581200</td>\n","      <td>3908.217800</td>\n","      <td>4863.4917</td>\n","      <td>1994.662000</td>\n","      <td>4122.034000</td>\n","      <td>1401.648400</td>\n","      <td>...</td>\n","      <td>5925.289000</td>\n","      <td>1091.123300</td>\n","      <td>1042.539700</td>\n","      <td>919.828250</td>\n","      <td>6455.265000</td>\n","      <td>2859.611800</td>\n","      <td>2799.460200</td>\n","      <td>4047.219200</td>\n","      <td>2059.743400</td>\n","      <td>2353.426300</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>2704.0930</td>\n","      <td>2079.968500</td>\n","      <td>2160.838100</td>\n","      <td>1938.232900</td>\n","      <td>1900.109400</td>\n","      <td>3699.395300</td>\n","      <td>4840.4300</td>\n","      <td>2016.958400</td>\n","      <td>3846.404300</td>\n","      <td>1469.242600</td>\n","      <td>...</td>\n","      <td>5867.860400</td>\n","      <td>1092.424000</td>\n","      <td>1058.137800</td>\n","      <td>844.960800</td>\n","      <td>6483.064500</td>\n","      <td>3133.451200</td>\n","      <td>2849.129400</td>\n","      <td>4029.520300</td>\n","      <td>2112.472700</td>\n","      <td>2427.927000</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>2649.7031</td>\n","      <td>2218.693400</td>\n","      <td>2025.289300</td>\n","      <td>1964.707800</td>\n","      <td>1833.210100</td>\n","      <td>3767.894800</td>\n","      <td>5226.5980</td>\n","      <td>1927.292100</td>\n","      <td>4043.276900</td>\n","      <td>1384.466200</td>\n","      <td>...</td>\n","      <td>5927.474000</td>\n","      <td>1138.479900</td>\n","      <td>1062.101400</td>\n","      <td>884.120200</td>\n","      <td>6437.127000</td>\n","      <td>3011.683800</td>\n","      <td>2899.210200</td>\n","      <td>4085.447500</td>\n","      <td>2228.401000</td>\n","      <td>2269.580600</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>2555.4036</td>\n","      <td>2234.234900</td>\n","      <td>2251.957500</td>\n","      <td>2011.093600</td>\n","      <td>2106.621300</td>\n","      <td>3911.747800</td>\n","      <td>4681.8060</td>\n","      <td>2178.151100</td>\n","      <td>3958.223400</td>\n","      <td>1530.339800</td>\n","      <td>...</td>\n","      <td>6610.960000</td>\n","      <td>1163.561200</td>\n","      <td>967.015200</td>\n","      <td>858.660500</td>\n","      <td>6474.855500</td>\n","      <td>2890.810500</td>\n","      <td>3012.551000</td>\n","      <td>4224.013000</td>\n","      <td>2038.122600</td>\n","      <td>2420.568600</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>2391.6406</td>\n","      <td>2110.986000</td>\n","      <td>2153.611000</td>\n","      <td>1871.428800</td>\n","      <td>1977.810400</td>\n","      <td>3897.885500</td>\n","      <td>4614.8125</td>\n","      <td>2018.013300</td>\n","      <td>4091.570300</td>\n","      <td>1400.770500</td>\n","      <td>...</td>\n","      <td>6305.104000</td>\n","      <td>1131.817500</td>\n","      <td>988.922500</td>\n","      <td>865.094670</td>\n","      <td>6404.954000</td>\n","      <td>2978.431600</td>\n","      <td>2805.287000</td>\n","      <td>4038.494000</td>\n","      <td>2140.035600</td>\n","      <td>2433.693600</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>2432.7588</td>\n","      <td>2141.061800</td>\n","      <td>2307.670400</td>\n","      <td>1871.195800</td>\n","      <td>1960.904000</td>\n","      <td>3748.895300</td>\n","      <td>5084.1300</td>\n","      <td>2065.085700</td>\n","      <td>4101.983000</td>\n","      <td>1432.151900</td>\n","      <td>...</td>\n","      <td>6432.797000</td>\n","      <td>1158.585300</td>\n","      <td>1106.671000</td>\n","      <td>861.926640</td>\n","      <td>6068.854500</td>\n","      <td>2832.755900</td>\n","      <td>3057.544700</td>\n","      <td>3857.684000</td>\n","      <td>2061.735000</td>\n","      <td>2288.010500</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>2307.2695</td>\n","      <td>2209.284700</td>\n","      <td>2094.340600</td>\n","      <td>2042.292000</td>\n","      <td>1909.829500</td>\n","      <td>3837.052000</td>\n","      <td>4928.9546</td>\n","      <td>1981.891200</td>\n","      <td>4018.949500</td>\n","      <td>1497.190000</td>\n","      <td>...</td>\n","      <td>6133.165500</td>\n","      <td>1174.436600</td>\n","      <td>1046.371500</td>\n","      <td>852.156860</td>\n","      <td>6036.931000</td>\n","      <td>2983.566200</td>\n","      <td>2800.511700</td>\n","      <td>3956.231700</td>\n","      <td>2061.298600</td>\n","      <td>2418.277000</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>2512.4536</td>\n","      <td>2184.163600</td>\n","      <td>2289.780300</td>\n","      <td>1964.576700</td>\n","      <td>2117.789000</td>\n","      <td>3819.671400</td>\n","      <td>4917.8460</td>\n","      <td>1953.171800</td>\n","      <td>4121.334000</td>\n","      <td>1516.695700</td>\n","      <td>...</td>\n","      <td>6129.551300</td>\n","      <td>1188.180900</td>\n","      <td>974.227100</td>\n","      <td>822.248540</td>\n","      <td>5952.837000</td>\n","      <td>2793.019500</td>\n","      <td>2990.980000</td>\n","      <td>3929.864300</td>\n","      <td>1951.214700</td>\n","      <td>2372.516600</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>2380.7197</td>\n","      <td>2213.407700</td>\n","      <td>2263.250500</td>\n","      <td>1924.683700</td>\n","      <td>1943.427600</td>\n","      <td>3780.757300</td>\n","      <td>4447.8190</td>\n","      <td>1940.095800</td>\n","      <td>3996.610800</td>\n","      <td>1489.984700</td>\n","      <td>...</td>\n","      <td>6062.897000</td>\n","      <td>1071.564600</td>\n","      <td>1048.462900</td>\n","      <td>887.632140</td>\n","      <td>6178.382000</td>\n","      <td>2903.879200</td>\n","      <td>2917.556400</td>\n","      <td>4037.454800</td>\n","      <td>2000.983200</td>\n","      <td>2380.047000</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>2472.6868</td>\n","      <td>2150.970000</td>\n","      <td>2196.457000</td>\n","      <td>1863.737800</td>\n","      <td>1957.257400</td>\n","      <td>3796.887500</td>\n","      <td>4966.7200</td>\n","      <td>1990.416400</td>\n","      <td>3998.694000</td>\n","      <td>1487.574100</td>\n","      <td>...</td>\n","      <td>6318.098600</td>\n","      <td>1155.526500</td>\n","      <td>1033.767700</td>\n","      <td>859.979900</td>\n","      <td>6597.917000</td>\n","      <td>2832.113500</td>\n","      <td>2825.306400</td>\n","      <td>3905.120400</td>\n","      <td>1987.797900</td>\n","      <td>2354.281200</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>2643.5916</td>\n","      <td>2109.740200</td>\n","      <td>2213.871600</td>\n","      <td>1923.724600</td>\n","      <td>1944.765300</td>\n","      <td>3561.996800</td>\n","      <td>4661.8784</td>\n","      <td>2096.163600</td>\n","      <td>3747.636500</td>\n","      <td>1468.624000</td>\n","      <td>...</td>\n","      <td>6420.741700</td>\n","      <td>1143.257700</td>\n","      <td>1024.603400</td>\n","      <td>901.389160</td>\n","      <td>5931.353000</td>\n","      <td>2776.784400</td>\n","      <td>2815.636200</td>\n","      <td>3878.844500</td>\n","      <td>2161.270300</td>\n","      <td>2262.784200</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>2429.3790</td>\n","      <td>1958.375600</td>\n","      <td>2217.941000</td>\n","      <td>2040.392700</td>\n","      <td>1954.205200</td>\n","      <td>3777.925500</td>\n","      <td>4486.6080</td>\n","      <td>1937.652100</td>\n","      <td>4024.625000</td>\n","      <td>1489.283000</td>\n","      <td>...</td>\n","      <td>6633.215300</td>\n","      <td>1022.997400</td>\n","      <td>1038.949800</td>\n","      <td>862.217800</td>\n","      <td>6649.317400</td>\n","      <td>2749.428500</td>\n","      <td>2866.756600</td>\n","      <td>4287.067000</td>\n","      <td>2179.608200</td>\n","      <td>2389.936800</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>2495.6047</td>\n","      <td>2192.069800</td>\n","      <td>2254.957500</td>\n","      <td>1994.835300</td>\n","      <td>1974.915200</td>\n","      <td>3837.778800</td>\n","      <td>4585.0557</td>\n","      <td>2060.752000</td>\n","      <td>4310.134300</td>\n","      <td>1516.349900</td>\n","      <td>...</td>\n","      <td>5894.157000</td>\n","      <td>1150.599600</td>\n","      <td>996.019600</td>\n","      <td>910.999500</td>\n","      <td>6393.194300</td>\n","      <td>2922.202000</td>\n","      <td>2933.635700</td>\n","      <td>3779.452600</td>\n","      <td>2002.221100</td>\n","      <td>2318.298800</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>2479.6357</td>\n","      <td>2081.817000</td>\n","      <td>2192.535400</td>\n","      <td>1971.280600</td>\n","      <td>1947.920000</td>\n","      <td>4193.608000</td>\n","      <td>4446.5190</td>\n","      <td>2028.518700</td>\n","      <td>3901.384300</td>\n","      <td>1541.405600</td>\n","      <td>...</td>\n","      <td>5885.730500</td>\n","      <td>1055.851200</td>\n","      <td>1064.131800</td>\n","      <td>879.676800</td>\n","      <td>6403.764000</td>\n","      <td>2985.828100</td>\n","      <td>3026.011000</td>\n","      <td>3899.249000</td>\n","      <td>2075.383800</td>\n","      <td>2281.430700</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>2640.9678</td>\n","      <td>2134.210200</td>\n","      <td>2181.424300</td>\n","      <td>2110.755100</td>\n","      <td>1912.853600</td>\n","      <td>3749.915000</td>\n","      <td>4089.6287</td>\n","      <td>1939.439600</td>\n","      <td>4060.610800</td>\n","      <td>1338.270100</td>\n","      <td>...</td>\n","      <td>6376.803000</td>\n","      <td>1110.459400</td>\n","      <td>1058.477200</td>\n","      <td>853.317570</td>\n","      <td>6546.675300</td>\n","      <td>2901.989300</td>\n","      <td>3010.852000</td>\n","      <td>3581.418500</td>\n","      <td>2091.183600</td>\n","      <td>2343.295700</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>2503.6467</td>\n","      <td>1953.460000</td>\n","      <td>2260.532500</td>\n","      <td>2025.342400</td>\n","      <td>1934.826800</td>\n","      <td>3822.831000</td>\n","      <td>4340.5530</td>\n","      <td>1922.243400</td>\n","      <td>3970.466300</td>\n","      <td>1451.545200</td>\n","      <td>...</td>\n","      <td>6046.294400</td>\n","      <td>1066.853100</td>\n","      <td>1028.372600</td>\n","      <td>852.377870</td>\n","      <td>6372.842300</td>\n","      <td>2965.545400</td>\n","      <td>3170.115200</td>\n","      <td>3656.383300</td>\n","      <td>2051.069000</td>\n","      <td>2217.139400</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>2507.5457</td>\n","      <td>2106.150600</td>\n","      <td>2228.801000</td>\n","      <td>1947.157600</td>\n","      <td>1909.163200</td>\n","      <td>3850.791300</td>\n","      <td>4099.4350</td>\n","      <td>2100.353000</td>\n","      <td>4090.979500</td>\n","      <td>1519.749300</td>\n","      <td>...</td>\n","      <td>5927.823000</td>\n","      <td>1108.419400</td>\n","      <td>1041.684800</td>\n","      <td>896.412100</td>\n","      <td>6218.843300</td>\n","      <td>2891.192100</td>\n","      <td>3015.943000</td>\n","      <td>3882.592800</td>\n","      <td>2069.763400</td>\n","      <td>2427.455800</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>2414.2263</td>\n","      <td>2088.529800</td>\n","      <td>2334.534400</td>\n","      <td>1986.702500</td>\n","      <td>1999.411900</td>\n","      <td>3791.733400</td>\n","      <td>3943.8850</td>\n","      <td>1881.341600</td>\n","      <td>4027.901000</td>\n","      <td>1421.372400</td>\n","      <td>...</td>\n","      <td>6095.906000</td>\n","      <td>1058.309400</td>\n","      <td>1042.541900</td>\n","      <td>867.276730</td>\n","      <td>6158.952000</td>\n","      <td>3017.944000</td>\n","      <td>3144.766400</td>\n","      <td>3983.404500</td>\n","      <td>2163.130000</td>\n","      <td>2111.729500</td>\n","    </tr>\n","    <tr>\n","      <th>25</th>\n","      <td>2631.5017</td>\n","      <td>2174.263200</td>\n","      <td>2231.679000</td>\n","      <td>2100.790500</td>\n","      <td>1989.730600</td>\n","      <td>3887.336200</td>\n","      <td>4239.7410</td>\n","      <td>2016.432700</td>\n","      <td>3820.346700</td>\n","      <td>1447.595500</td>\n","      <td>...</td>\n","      <td>5825.999500</td>\n","      <td>1071.444500</td>\n","      <td>1053.347800</td>\n","      <td>866.366500</td>\n","      <td>5942.574700</td>\n","      <td>2865.497000</td>\n","      <td>3003.416300</td>\n","      <td>3772.822300</td>\n","      <td>1976.094500</td>\n","      <td>2398.749300</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>2667.8171</td>\n","      <td>2068.444800</td>\n","      <td>2082.848400</td>\n","      <td>2069.827000</td>\n","      <td>2070.345500</td>\n","      <td>3840.469500</td>\n","      <td>4038.7314</td>\n","      <td>1870.766600</td>\n","      <td>3762.023700</td>\n","      <td>1423.111500</td>\n","      <td>...</td>\n","      <td>6272.114300</td>\n","      <td>1111.838700</td>\n","      <td>1133.561500</td>\n","      <td>854.553100</td>\n","      <td>6036.143600</td>\n","      <td>2927.757800</td>\n","      <td>2996.377400</td>\n","      <td>3870.803700</td>\n","      <td>2012.912400</td>\n","      <td>2442.134500</td>\n","    </tr>\n","    <tr>\n","      <th>27</th>\n","      <td>2509.6267</td>\n","      <td>2223.662000</td>\n","      <td>2214.927700</td>\n","      <td>1879.381700</td>\n","      <td>2034.301800</td>\n","      <td>3990.701700</td>\n","      <td>3722.9275</td>\n","      <td>1796.140000</td>\n","      <td>3963.283200</td>\n","      <td>1413.631600</td>\n","      <td>...</td>\n","      <td>5681.063500</td>\n","      <td>1160.893400</td>\n","      <td>980.073550</td>\n","      <td>876.500200</td>\n","      <td>6292.896500</td>\n","      <td>2778.773400</td>\n","      <td>3324.061000</td>\n","      <td>3887.803200</td>\n","      <td>1928.287400</td>\n","      <td>2270.807100</td>\n","    </tr>\n","    <tr>\n","      <th>28</th>\n","      <td>2504.5080</td>\n","      <td>2048.426300</td>\n","      <td>2109.233400</td>\n","      <td>1821.881800</td>\n","      <td>1896.296100</td>\n","      <td>3761.522700</td>\n","      <td>3550.6020</td>\n","      <td>1749.937100</td>\n","      <td>4247.510700</td>\n","      <td>1456.532000</td>\n","      <td>...</td>\n","      <td>6353.352000</td>\n","      <td>1109.217900</td>\n","      <td>1023.777950</td>\n","      <td>879.716700</td>\n","      <td>6470.217300</td>\n","      <td>3073.331300</td>\n","      <td>2917.789600</td>\n","      <td>3984.705000</td>\n","      <td>2262.620800</td>\n","      <td>2133.361800</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>29 rows × 37 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c6512d95-6540-47c6-8c04-b575af60d1c5')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-c6512d95-6540-47c6-8c04-b575af60d1c5 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-c6512d95-6540-47c6-8c04-b575af60d1c5');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["date = [f'd+{i}' for i in range(1,15)] + ['d+22 ~ 28 평균']\n","\n","\n","for k in range(10):\n","  globals()[f'answer_df_{k}'] = pd.DataFrame()\n","  for c in globals()[f'set_df_{k}'].columns:\n","    base_d = globals()[f'set_df_{k}'][c][0] # 변동률 기준 t 값\n","\n","    ans_1_14 = []\n","    for i in range(14):\n","      ans_1_14.append((globals()[f'set_df_{k}'][c].iloc[i+1]- base_d)/base_d)  # t+1 ~ t+14 까지는 (t+n - t)/t 로 계산\n","\n","    ans_22_28 = (globals()[f'set_df_{k}'][c][22:29].mean() - base_d)/base_d # t+22 ~ t+28은 np.mean(t+22 ~ t+28) - t / t\n","\n","    globals()[f'answer_df_{k}'][f'{c} 변동률'] = ans_1_14 + [ans_22_28]\n","  \n","  globals()[f'answer_df_{k}']['Set'] = k # set 번호 설정\n","  globals()[f'answer_df_{k}']['일자'] = date # 일자 설정"],"metadata":{"id":"we8wEw0fm4qW","executionInfo":{"status":"ok","timestamp":1662482555305,"user_tz":-540,"elapsed":943,"user":{"displayName":"김성찬","userId":"03517507033102912155"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["# 위에서 계산된 변동률 들을 합쳐주는 과정\n","\n","all_df =pd.DataFrame()\n","for i in range(10):\n","  if i== 0 :\n","    all_df = pd.concat([all_df, globals()[f'answer_df_{i}']],axis=1)\n","  else:\n","    all_df = pd.concat([all_df, globals()[f'answer_df_{i}']])\n","\n","\n","all_df = all_df[['Set','일자'] + list(all_df.columns[:-2])]\n","all_df.reset_index(drop=True, inplace=True)"],"metadata":{"id":"S6Q1dY_Km4i8","executionInfo":{"status":"ok","timestamp":1662482557267,"user_tz":-540,"elapsed":474,"user":{"displayName":"김성찬","userId":"03517507033102912155"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["# set, 일자 기억하기위해 따로 저장\n","\n","re_set = list(all_df['Set'])\n","re_date = list(all_df['일자'])\n","\n","\n","# 정답 양식 불러오기\n","out_ans = pd.read_csv('./answer_example.csv')\n","\n","# 두 dataframe 합치기 (nan + 숫자 = nan 이용)\n","submit_df = all_df + out_ans\n","\n","submit_df['Set'] = re_set\n","submit_df['일자'] = re_date\n","\n","\n","# 최종 저장\n","submit_df.to_csv('./submit.csv',index=False)"],"metadata":{"id":"YRkQowv_m8ps","executionInfo":{"status":"ok","timestamp":1662482647872,"user_tz":-540,"elapsed":1344,"user":{"displayName":"김성찬","userId":"03517507033102912155"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Rb_DM_2pm8ZK"},"execution_count":null,"outputs":[]}]}